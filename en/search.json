[
  {
    "objectID": "pages/formations/git.html",
    "href": "pages/formations/git.html",
    "title": "Git training",
    "section": "",
    "text": "Git training\nThe sources of the Git training are available on GitHub. The training is accessible below.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Useful links",
      "Training",
      "Git training"
    ]
  },
  {
    "objectID": "pages/formations/tidyverse.html",
    "href": "pages/formations/tidyverse.html",
    "title": "Tidyverse training",
    "section": "",
    "text": "Tidyverse training\n\n\nThe source code for the tidyverse training course is available on the GitHub repository.\nThe associated presentation is available at the following address.\nThe last session took place on March 3 and 4, 2025.\nDetails of future sessions will be announced at a later date.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Useful links",
      "Training",
      "Tidyverse training"
    ]
  },
  {
    "objectID": "pages/serveurs/marbec_gpu/run_r_script.html",
    "href": "pages/serveurs/marbec_gpu/run_r_script.html",
    "title": "Running R scripts in marbec-gpu",
    "section": "",
    "text": "As when working with RStudio locally (i.e. on our PC), it is recommended to clearly define our working directory. This is extremely important because any process we run (either from RStudio or Terminal) will use that directory as a reference to find input files, output files or even other scripts.\nFor our case, we have created a folder called mgpu-examples/ where there is a subfolder called code/. The creation of folders in marbec-data can be done directly from the web interface (by clicking on File station and then using the Create folder button), the command mkdir, but we can also copy-paste the elements already existing in our PC into the working folder.\n\n\n\nThe following is NOT mandatory, but very useful, especially when working with RStudio and that is to create an RStudio project. To do this, we will go to File and then New Project.\n\nThen, in the window that appears, click on Existing directory, then on Browse and click on the folder that we have defined as our working directory (in our case, mgpu-examples/). Then, OK and finally click on the Create Project button. Rstudio will flicker a little bit and then will show us the same window, but inside the set project. The easiest way to check that the project has been created in the correct folder (mgpu-examples/ in our case) is to verify that right in the Console panel, to the right of the R version, appears only the path of our main folder (and not any of the subfolders, e.g. mgpu-examples/code/ or mgpu-examples/inputs/).\n\n\n\n\n\n\n\nJust before to say hello\n\n\n\nmarbec-gpu incorporates the possibility of working with RStudio (Server); however, this interface should be used ONLY to PREPARE our scripts before being executed using all the power of our server. In other words, within the RStudio environment we will be able to load not so big files and perform basic operations, but at no time should we execute a complex (heavy) process from there, but from Terminal.\n\n\n\n\n\n\n\n\nWe will start with the simplest: create a script in R and print the (very famous) “Hello world!” message.\n\nWe will start by opening an RStudio session from the JupyterLab environment (if you want to know how to get there, check the post of Introduction to marbec-gpu).\nOnce inside the RStudio environment, we will create a new script (File -&gt; New file -&gt; R script) which will contain a single line:\n\n\nprint(\"Hello world and hello marbec-gpu!\")\n\nThen, we will save that script with the name code/ex1-1.R (code/ refers to a subfolder created previously inside the working directory of our project in RStudio).\nNow comes the interesting part, inside our browser, we must go back to the Launcher tab and open a Terminal window (clicking on the corresponding icon).\nBy default, Terminal will open a session in the local folder assigned to our user. From there, we must get to the folder we have set as working directory; that is, the folder that our script will recognize as working directory (whether we have decided to use RStudio or not to create it or create a project inside it). Assuming that our working directory is the mgpu-examples/ folder, we must reach it using the cd command:\n\ncd mgpu-examples/\n\n\n\n\n\n\nHow do we know that we have arrived at the correct folder?\n\n\n\nFirst, the prompt will indicate the name of the folder in which it is located.\n\nIn addition, we can run the ls command which will show the subfolders and files inside the folder we have reached. If everything matches, then we did well.\n\n\n\n\nNext, we execute the following command in the Terminal: Rscript code/ex1-1.R and the result should be just what would be shown in a usual R session.\n\n\n\n\n\nIn this next example, we will show a script that generates and saves files in our working directory where previously, we will create two new folders (figures/ and outputs/) through the mkdir command as follows:\nmkdir figures/ outputs/\n\n\n\n\n\n\nNote\n\n\n\nWithin the Terminal environment, it is not possible to observe graphics interactively (as in RStudio), so if you want to keep any figure, you must always include the code to save it within the script you execute. Depending on the graphical environment, we can use functions such as png, bmp, jpeg, pdf (for graphics environment), or ggsave (for ggplot2 environment).\n\n\n\nNow, let’s go to RStudio to create the following script and save it in code/ex1-2.R:\n\n# Print mtcars\nprint(mtcars)\n\n# Export mtcars as a csv\nwrite.csv(x = mtcars, file = \"outputs/mtcars.csv\")\n \n# Create and save a scatterplot\npng(filename = \"figures/fig_1-1.png\")\n\nplot(x = mtcars$mpg, y = mtcars$disp, \n     xlab = \"Miles per (US) gallon\", ylab = \"Displacement (cu.in.)\")\n\ndev.off()\n\nNext, we go back to the Terminal environment and run our new script with the command Rscript code/ex1-2.R. Immediately, the mtcars table will be displayed as that is what the first line of our script commands.\n\n\n\nHowever, if we run the ls command in Terminal for the figures/ and outputs/ folders, we will see that the two files we ordered to be created inside our script appear.\n\n\n\nIf the files created are the ones we expect to collect from our analysis, we can download them through Filezilla (see the corresponding post).\n\n\n\n\n\n\n\nPreviewing figures\n\n\n\nWhile it is not possible to preview figures in Terminal or JupyterLab because they do not have an image viewer, it is possible to do so from the marbec-data web environment. However, this is a basic viewer and only available for the most common file types.\n\n\n\n\n\n\n\n\n\nWe will start by creating a script (which we will save as code/ex2-1.R) containing a simple loop that generates 20 100x100 arrays with random values and saves them in separate csv files inside the outputs/ex2-rndmats/ folder (remember to create that folder beforehand using mkdir):\n\n# Setting number of rows and columns\nrow_n &lt;- 100\ncol_n &lt;- 100\n\nfor(i in seq(20)){\n  # Create random matrix\n  rndMat &lt;- matrix(data = runif(n = row_n*col_n), nrow = row_n, ncol = col_n)\n  \n  # Save matrix\n  write.csv(x = rndMat, \n            file = sprintf(fmt = \"outputs/ex2-rndmats/mat_%02d.csv\", i), \n            row.names = FALSE)\n  \n  # Print a message at the end of each step\n  cat(sprintf(fmt = \"Matrix %02d finished!\\n\", i))\n}\n\nNow, we will run our script in Terminal (with the command Rscript code/ex2-1.R) and we will observe that everything went well if the messages at the end of each step of the loop are displayed correctly and also if when we run the command ls on the target folder we see the files created:\n\n\n\n\n\n\n\n\nRun a small example first\n\n\n\nBeing already in a real execution, it is highly recommended always to try with a small example that allows us to corroborate that our script goes well BEFORE to pull out all the stops trying to execute the heavy process. In addition, if our script returns figures or files, executing a small corroboration script allows us to quickly check if the generated files are consistent with what we expect to obtain.\n\n\n\n\n\n\nStarting from the previous example, we will convert our script into one that executes the processes in parallel. For this we will take advantage of the tools of the packages foreach and doParallel. Note that the names of the files of this script will begin with the letters mc_ to be able to recognize them with respect to those obtained in the previous example:\n\n# Setting number of rows and columns\nrow_n &lt;- 100\ncol_n &lt;- 100\n\nrequire(foreach)\nrequire(doParallel)\n\n# Registering cluster\ncl &lt;- makeCluster(spec = 20)\nregisterDoParallel(cl = cl)\n\n# Run multithread process\nout &lt;- foreach(i = seq(20), .inorder = FALSE) %dopar% {\n# Create random matrix\n  rndMat &lt;- matrix(data = runif(n = row_n*col_n), nrow = row_n, ncol = col_n)\n  \n  # Save matrix\n  write.csv(x = rndMat, \n            file = sprintf(fmt = \"outputs/ex2-rndmats/mc_mat_%02d.csv\", i), \n            row.names = FALSE)\n  \n  NULL\n}\n\n# Finish cluster\nstopCluster(cl)\n\nNow, we will run our script in Terminal (with the command Rscript code/ex2-2.R) and we will observe that everything has gone well if when executing the command ls on the target folder we see the created files:\n\n\n\n\n\n\n\n\nNote\n\n\n\nA couple of things:\n\nIn the script of the second example, foreach is assigned to an object (out) which will receive the last object generated within each step of the loop. If you only want to get files to be exported (figures, tables, NetCDF, etc.), be sure to leave a NULL in the last line of the loop. On the other hand, if you want to get an object and it is placed in that position, foreach will compile it using the list function, i.e. the final object (out) will be a list that will have as many levels as there are steps in the loop. Also, it is important to note that internally foreach runs a separate small R session so it is necessary to indicate the additional packages required through the .packages argument (see the following example).\nThe argument spec = 20 inside makeCluster refers to the amount of threads that will be used to execute the loop. Remember that one of the options when creating your server in marbec-gpu was to choose the amount of CPUs (2, 4, 8, 16, 32…)? Well, it is precisely with this argument where you will indicate that amount of logical cores. Remember that another important aspect is the RAM. At a given time each process running within each thread will have to load everything that a single simple process would need. In other words, if in a single core process, in each step of our loop we have to load 5 NetCDF files that occupy 5 GB in RAM, if we run that process in multicore and we define spec = 40, at a given moment we will have to load 5GBx40 (200 GB) in RAM simultaneously. So not only you must choose well the configuration of your server (regarding the script you plan to run), but also an approximate of what is consumed in each independent process, in order not to saturate your server. marbec-gpu is great, but it has its limits.",
    "crumbs": [
      "Useful links",
      "Servers",
      "Marbec GPU",
      "R Scripts Running"
    ]
  },
  {
    "objectID": "pages/serveurs/marbec_gpu/run_r_script.html#lets-tidy-up-a-bit",
    "href": "pages/serveurs/marbec_gpu/run_r_script.html#lets-tidy-up-a-bit",
    "title": "Running R scripts in marbec-gpu",
    "section": "",
    "text": "As when working with RStudio locally (i.e. on our PC), it is recommended to clearly define our working directory. This is extremely important because any process we run (either from RStudio or Terminal) will use that directory as a reference to find input files, output files or even other scripts.\nFor our case, we have created a folder called mgpu-examples/ where there is a subfolder called code/. The creation of folders in marbec-data can be done directly from the web interface (by clicking on File station and then using the Create folder button), the command mkdir, but we can also copy-paste the elements already existing in our PC into the working folder.\n\n\n\nThe following is NOT mandatory, but very useful, especially when working with RStudio and that is to create an RStudio project. To do this, we will go to File and then New Project.\n\nThen, in the window that appears, click on Existing directory, then on Browse and click on the folder that we have defined as our working directory (in our case, mgpu-examples/). Then, OK and finally click on the Create Project button. Rstudio will flicker a little bit and then will show us the same window, but inside the set project. The easiest way to check that the project has been created in the correct folder (mgpu-examples/ in our case) is to verify that right in the Console panel, to the right of the R version, appears only the path of our main folder (and not any of the subfolders, e.g. mgpu-examples/code/ or mgpu-examples/inputs/).\n\n\n\n\n\n\n\nJust before to say hello\n\n\n\nmarbec-gpu incorporates the possibility of working with RStudio (Server); however, this interface should be used ONLY to PREPARE our scripts before being executed using all the power of our server. In other words, within the RStudio environment we will be able to load not so big files and perform basic operations, but at no time should we execute a complex (heavy) process from there, but from Terminal.",
    "crumbs": [
      "Useful links",
      "Servers",
      "Marbec GPU",
      "R Scripts Running"
    ]
  },
  {
    "objectID": "pages/serveurs/marbec_gpu/run_r_script.html#hello-world-simple-examples",
    "href": "pages/serveurs/marbec_gpu/run_r_script.html#hello-world-simple-examples",
    "title": "Running R scripts in marbec-gpu",
    "section": "",
    "text": "We will start with the simplest: create a script in R and print the (very famous) “Hello world!” message.\n\nWe will start by opening an RStudio session from the JupyterLab environment (if you want to know how to get there, check the post of Introduction to marbec-gpu).\nOnce inside the RStudio environment, we will create a new script (File -&gt; New file -&gt; R script) which will contain a single line:\n\n\nprint(\"Hello world and hello marbec-gpu!\")\n\nThen, we will save that script with the name code/ex1-1.R (code/ refers to a subfolder created previously inside the working directory of our project in RStudio).\nNow comes the interesting part, inside our browser, we must go back to the Launcher tab and open a Terminal window (clicking on the corresponding icon).\nBy default, Terminal will open a session in the local folder assigned to our user. From there, we must get to the folder we have set as working directory; that is, the folder that our script will recognize as working directory (whether we have decided to use RStudio or not to create it or create a project inside it). Assuming that our working directory is the mgpu-examples/ folder, we must reach it using the cd command:\n\ncd mgpu-examples/\n\n\n\n\n\n\nHow do we know that we have arrived at the correct folder?\n\n\n\nFirst, the prompt will indicate the name of the folder in which it is located.\n\nIn addition, we can run the ls command which will show the subfolders and files inside the folder we have reached. If everything matches, then we did well.\n\n\n\n\nNext, we execute the following command in the Terminal: Rscript code/ex1-1.R and the result should be just what would be shown in a usual R session.\n\n\n\n\n\nIn this next example, we will show a script that generates and saves files in our working directory where previously, we will create two new folders (figures/ and outputs/) through the mkdir command as follows:\nmkdir figures/ outputs/\n\n\n\n\n\n\nNote\n\n\n\nWithin the Terminal environment, it is not possible to observe graphics interactively (as in RStudio), so if you want to keep any figure, you must always include the code to save it within the script you execute. Depending on the graphical environment, we can use functions such as png, bmp, jpeg, pdf (for graphics environment), or ggsave (for ggplot2 environment).\n\n\n\nNow, let’s go to RStudio to create the following script and save it in code/ex1-2.R:\n\n# Print mtcars\nprint(mtcars)\n\n# Export mtcars as a csv\nwrite.csv(x = mtcars, file = \"outputs/mtcars.csv\")\n \n# Create and save a scatterplot\npng(filename = \"figures/fig_1-1.png\")\n\nplot(x = mtcars$mpg, y = mtcars$disp, \n     xlab = \"Miles per (US) gallon\", ylab = \"Displacement (cu.in.)\")\n\ndev.off()\n\nNext, we go back to the Terminal environment and run our new script with the command Rscript code/ex1-2.R. Immediately, the mtcars table will be displayed as that is what the first line of our script commands.\n\n\n\nHowever, if we run the ls command in Terminal for the figures/ and outputs/ folders, we will see that the two files we ordered to be created inside our script appear.\n\n\n\nIf the files created are the ones we expect to collect from our analysis, we can download them through Filezilla (see the corresponding post).\n\n\n\n\n\n\n\nPreviewing figures\n\n\n\nWhile it is not possible to preview figures in Terminal or JupyterLab because they do not have an image viewer, it is possible to do so from the marbec-data web environment. However, this is a basic viewer and only available for the most common file types.",
    "crumbs": [
      "Useful links",
      "Servers",
      "Marbec GPU",
      "R Scripts Running"
    ]
  },
  {
    "objectID": "pages/serveurs/marbec_gpu/run_r_script.html#hello-universe-parallel-process",
    "href": "pages/serveurs/marbec_gpu/run_r_script.html#hello-universe-parallel-process",
    "title": "Running R scripts in marbec-gpu",
    "section": "",
    "text": "We will start by creating a script (which we will save as code/ex2-1.R) containing a simple loop that generates 20 100x100 arrays with random values and saves them in separate csv files inside the outputs/ex2-rndmats/ folder (remember to create that folder beforehand using mkdir):\n\n# Setting number of rows and columns\nrow_n &lt;- 100\ncol_n &lt;- 100\n\nfor(i in seq(20)){\n  # Create random matrix\n  rndMat &lt;- matrix(data = runif(n = row_n*col_n), nrow = row_n, ncol = col_n)\n  \n  # Save matrix\n  write.csv(x = rndMat, \n            file = sprintf(fmt = \"outputs/ex2-rndmats/mat_%02d.csv\", i), \n            row.names = FALSE)\n  \n  # Print a message at the end of each step\n  cat(sprintf(fmt = \"Matrix %02d finished!\\n\", i))\n}\n\nNow, we will run our script in Terminal (with the command Rscript code/ex2-1.R) and we will observe that everything went well if the messages at the end of each step of the loop are displayed correctly and also if when we run the command ls on the target folder we see the files created:\n\n\n\n\n\n\n\n\nRun a small example first\n\n\n\nBeing already in a real execution, it is highly recommended always to try with a small example that allows us to corroborate that our script goes well BEFORE to pull out all the stops trying to execute the heavy process. In addition, if our script returns figures or files, executing a small corroboration script allows us to quickly check if the generated files are consistent with what we expect to obtain.\n\n\n\n\n\n\nStarting from the previous example, we will convert our script into one that executes the processes in parallel. For this we will take advantage of the tools of the packages foreach and doParallel. Note that the names of the files of this script will begin with the letters mc_ to be able to recognize them with respect to those obtained in the previous example:\n\n# Setting number of rows and columns\nrow_n &lt;- 100\ncol_n &lt;- 100\n\nrequire(foreach)\nrequire(doParallel)\n\n# Registering cluster\ncl &lt;- makeCluster(spec = 20)\nregisterDoParallel(cl = cl)\n\n# Run multithread process\nout &lt;- foreach(i = seq(20), .inorder = FALSE) %dopar% {\n# Create random matrix\n  rndMat &lt;- matrix(data = runif(n = row_n*col_n), nrow = row_n, ncol = col_n)\n  \n  # Save matrix\n  write.csv(x = rndMat, \n            file = sprintf(fmt = \"outputs/ex2-rndmats/mc_mat_%02d.csv\", i), \n            row.names = FALSE)\n  \n  NULL\n}\n\n# Finish cluster\nstopCluster(cl)\n\nNow, we will run our script in Terminal (with the command Rscript code/ex2-2.R) and we will observe that everything has gone well if when executing the command ls on the target folder we see the created files:\n\n\n\n\n\n\n\n\nNote\n\n\n\nA couple of things:\n\nIn the script of the second example, foreach is assigned to an object (out) which will receive the last object generated within each step of the loop. If you only want to get files to be exported (figures, tables, NetCDF, etc.), be sure to leave a NULL in the last line of the loop. On the other hand, if you want to get an object and it is placed in that position, foreach will compile it using the list function, i.e. the final object (out) will be a list that will have as many levels as there are steps in the loop. Also, it is important to note that internally foreach runs a separate small R session so it is necessary to indicate the additional packages required through the .packages argument (see the following example).\nThe argument spec = 20 inside makeCluster refers to the amount of threads that will be used to execute the loop. Remember that one of the options when creating your server in marbec-gpu was to choose the amount of CPUs (2, 4, 8, 16, 32…)? Well, it is precisely with this argument where you will indicate that amount of logical cores. Remember that another important aspect is the RAM. At a given time each process running within each thread will have to load everything that a single simple process would need. In other words, if in a single core process, in each step of our loop we have to load 5 NetCDF files that occupy 5 GB in RAM, if we run that process in multicore and we define spec = 40, at a given moment we will have to load 5GBx40 (200 GB) in RAM simultaneously. So not only you must choose well the configuration of your server (regarding the script you plan to run), but also an approximate of what is consumed in each independent process, in order not to saturate your server. marbec-gpu is great, but it has its limits.",
    "crumbs": [
      "Useful links",
      "Servers",
      "Marbec GPU",
      "R Scripts Running"
    ]
  },
  {
    "objectID": "pages/serveurs/marbec_gpu/index_marbec_gpu.html",
    "href": "pages/serveurs/marbec_gpu/index_marbec_gpu.html",
    "title": "Marbec-GPU Documentation",
    "section": "",
    "text": "Welcome to the Marbec-GPU cluster documentation. This document provides an overview of the cluster, its capabilities, and how to get started with using it.\nThe Marbec-GPU cluster is designed to provide high-performance computing resources for code execution, such as those using Python and R. It is built on the Linux-Ubuntu kernel and features a Jupyter interface for ease of use. Several common tools are installed, including Python, R, Git, Conda, CUDA, and RStudio.\n\n\n\nRessources\n\n2 NVIDIA A40 GPUs\n2 Intel Xeon Platinum 8380 CPUs, 2x40 cores, 2x80 threads\n1,48 To de RAM\nMARBEC-DATA Interconnections\n\n\n\n\n\nTo start using the Marbec-GPU cluster, you will need to join the Marbec-DEN group. Contact the Administrators for more details : Contact DEN administrators.\n\n\n\nFor detailed instructions on how to use the Marbec-GPU cluster, please refer to the following sections:\n\nInitiation Guide (comming soon)\nUseful Linux Commands Guide\nBasic Script Execution (via SLURM)\nR Script Execution\n\n\n\n\nIf you encounter any issues or have questions interact with the RocketChatIRD.\n\n\n\n\nWhat resources do I need to allocate?\n\nGood question! It depends on your input data (size and type), your model (stochastic, statistical, neural network, etc.), your task, but most importantly, the packages you are using. For example, some packages do not support GPU computations, while others cannot parallelize across multiple CPUs. Make sure to research the packages you’re using to avoid allocating resources that won’t be utilized, and adapt your scripts accordingly. Here are some examples of resource allocation: Training Pytorch YOLO: --mem=64G, --c=16, and --gres=gpu:1; Running HSMC (TensorFlow): --mem=64GB, --cpus-per-task=30, and --gpus-per-node=1.\n\nDoes my script is GPU-capable ?\n\nNo, not directly. However, some libraries are GPU-capable. If your framework or script does not specifically use the GPU, your code will NOT utilize GPU hardware. Main examples of GPU-capable libraries: PyTorch, TensorFlow, Keras, Theano, Caffe, etc.\n\nHow to cancel a submitted job ?\n\nUse the command scancel JOBID, where JOBID is the job ID of the job you want to cancel. You can find the job ID in the output of the sbatch command when you submit a job, or by using the squeue command as mentioned in the previous question, for more details SLURM scancel documentation.\n\nHow access job queue ?\n\nUse the following command : squeue -O NAME,UserName,TimeUsed,tres-per-node,state,JOBID. This command displays a detailed list of jobs in the queue, including the job name (e.g., spawner-jupyterhub for a “job-session”; otherwise, the name specified in the #SBATCH --job-name argument), username, running time, node name (eg., gres:gpu:1 for a GPU allocation, gres:gpu:0 for a CPU allocation), job state (e.g., PENDING for jobs waiting to start due to resource availability or scheduling, or RUNNING for jobs currently being executed) and JOBID (a unique identifier for each job), refer to the SLURM squeue documentation for more details.\n\nHow to submit multiple jobs without blocking other users ?\n\nThank you from the entire MarbecGPU Community for using resources in a cooperative and friendly manner. You can use #SBATCH --dependency=afterany:JOBID parameter, where JOBID is the job ID of the job you want to wait for (e.g., 4391). You can find the job ID in the output of the sbatch command when you submit a job, or by using the squeue command as mentioned in the previous question. According to the SLURM sbatch documentation this parameter ensures that the start of your job is deferred until the specified dependency is satisfied. For file-based dependencies or more complex cases, you can explore other mechanisms to further delay or sequence your job execution as needed.",
    "crumbs": [
      "Useful links",
      "Servers",
      "Marbec GPU"
    ]
  },
  {
    "objectID": "pages/serveurs/marbec_gpu/index_marbec_gpu.html#features",
    "href": "pages/serveurs/marbec_gpu/index_marbec_gpu.html#features",
    "title": "Marbec-GPU Documentation",
    "section": "",
    "text": "Ressources\n\n2 NVIDIA A40 GPUs\n2 Intel Xeon Platinum 8380 CPUs, 2x40 cores, 2x80 threads\n1,48 To de RAM\nMARBEC-DATA Interconnections",
    "crumbs": [
      "Useful links",
      "Servers",
      "Marbec GPU"
    ]
  },
  {
    "objectID": "pages/serveurs/marbec_gpu/index_marbec_gpu.html#registration",
    "href": "pages/serveurs/marbec_gpu/index_marbec_gpu.html#registration",
    "title": "Marbec-GPU Documentation",
    "section": "",
    "text": "To start using the Marbec-GPU cluster, you will need to join the Marbec-DEN group. Contact the Administrators for more details : Contact DEN administrators.",
    "crumbs": [
      "Useful links",
      "Servers",
      "Marbec GPU"
    ]
  },
  {
    "objectID": "pages/serveurs/marbec_gpu/index_marbec_gpu.html#documentation-1",
    "href": "pages/serveurs/marbec_gpu/index_marbec_gpu.html#documentation-1",
    "title": "Marbec-GPU Documentation",
    "section": "",
    "text": "For detailed instructions on how to use the Marbec-GPU cluster, please refer to the following sections:\n\nInitiation Guide (comming soon)\nUseful Linux Commands Guide\nBasic Script Execution (via SLURM)\nR Script Execution",
    "crumbs": [
      "Useful links",
      "Servers",
      "Marbec GPU"
    ]
  },
  {
    "objectID": "pages/serveurs/marbec_gpu/index_marbec_gpu.html#support-1",
    "href": "pages/serveurs/marbec_gpu/index_marbec_gpu.html#support-1",
    "title": "Marbec-GPU Documentation",
    "section": "",
    "text": "If you encounter any issues or have questions interact with the RocketChatIRD.",
    "crumbs": [
      "Useful links",
      "Servers",
      "Marbec GPU"
    ]
  },
  {
    "objectID": "pages/serveurs/marbec_gpu/index_marbec_gpu.html#faq-1",
    "href": "pages/serveurs/marbec_gpu/index_marbec_gpu.html#faq-1",
    "title": "Marbec-GPU Documentation",
    "section": "",
    "text": "What resources do I need to allocate?\n\nGood question! It depends on your input data (size and type), your model (stochastic, statistical, neural network, etc.), your task, but most importantly, the packages you are using. For example, some packages do not support GPU computations, while others cannot parallelize across multiple CPUs. Make sure to research the packages you’re using to avoid allocating resources that won’t be utilized, and adapt your scripts accordingly. Here are some examples of resource allocation: Training Pytorch YOLO: --mem=64G, --c=16, and --gres=gpu:1; Running HSMC (TensorFlow): --mem=64GB, --cpus-per-task=30, and --gpus-per-node=1.\n\nDoes my script is GPU-capable ?\n\nNo, not directly. However, some libraries are GPU-capable. If your framework or script does not specifically use the GPU, your code will NOT utilize GPU hardware. Main examples of GPU-capable libraries: PyTorch, TensorFlow, Keras, Theano, Caffe, etc.\n\nHow to cancel a submitted job ?\n\nUse the command scancel JOBID, where JOBID is the job ID of the job you want to cancel. You can find the job ID in the output of the sbatch command when you submit a job, or by using the squeue command as mentioned in the previous question, for more details SLURM scancel documentation.\n\nHow access job queue ?\n\nUse the following command : squeue -O NAME,UserName,TimeUsed,tres-per-node,state,JOBID. This command displays a detailed list of jobs in the queue, including the job name (e.g., spawner-jupyterhub for a “job-session”; otherwise, the name specified in the #SBATCH --job-name argument), username, running time, node name (eg., gres:gpu:1 for a GPU allocation, gres:gpu:0 for a CPU allocation), job state (e.g., PENDING for jobs waiting to start due to resource availability or scheduling, or RUNNING for jobs currently being executed) and JOBID (a unique identifier for each job), refer to the SLURM squeue documentation for more details.\n\nHow to submit multiple jobs without blocking other users ?\n\nThank you from the entire MarbecGPU Community for using resources in a cooperative and friendly manner. You can use #SBATCH --dependency=afterany:JOBID parameter, where JOBID is the job ID of the job you want to wait for (e.g., 4391). You can find the job ID in the output of the sbatch command when you submit a job, or by using the squeue command as mentioned in the previous question. According to the SLURM sbatch documentation this parameter ensures that the start of your job is deferred until the specified dependency is satisfied. For file-based dependencies or more complex cases, you can explore other mechanisms to further delay or sequence your job execution as needed.",
    "crumbs": [
      "Useful links",
      "Servers",
      "Marbec GPU"
    ]
  },
  {
    "objectID": "pages/serveurs/marbec_gpu/intro_marbec_gpu.html",
    "href": "pages/serveurs/marbec_gpu/intro_marbec_gpu.html",
    "title": "Initiation to Marbec-GPU Cluster",
    "section": "",
    "text": "Initiation to Marbec-GPU Cluster\nA presentation video of the cluster will be available soon.\n\n\n\n\n Back to top",
    "crumbs": [
      "Useful links",
      "Servers",
      "Marbec GPU",
      "Initiation"
    ]
  },
  {
    "objectID": "pages/serveurs/index_serveurs.html",
    "href": "pages/serveurs/index_serveurs.html",
    "title": "Digital Ecology Scheme (DEN) Resources and Materials",
    "section": "",
    "text": "The DEN Marbec offers two servers, a MARBEC-GPU computing server and a MARBEC-DATA storage server. A collaborative documentation is being written to help you get familiar with these servers. You can consult the presentation pages of these servers and some basic tutorials to help you get started:\n\nMARBEC-GPU Intro\nMARBEC-GPU Tutorials\nMARBEC-DATA Intro\nManaging files on MARBEC-DATA\n\nAny user willing to contribute to the documentation or give feedback is welcome: how to contribute.\nEnjoy!\n\n\n\n Back to top",
    "crumbs": [
      "Useful links",
      "Servers"
    ]
  },
  {
    "objectID": "pages/serveurs/marbec_data/index_marbec_data.html",
    "href": "pages/serveurs/marbec_data/index_marbec_data.html",
    "title": "Introduction to marbec-data",
    "section": "",
    "text": "marbec-data is a NFS. A NFS is a network protocol that allows multiple devices connected to a network to share files and directories. This allows researchers to store input data, codes and results, but with the advantage of having a centralized backup and the ability to access their files from any machine connected to the cluster. In very simple words and going back to the analogy with your current PC, marbec-data takes the place of the storage (i.e. the hard disk) in the HPC. On the other hand, a compute cluster is, in essence, a set of interconnected computational elements working in a coordinated manner to execute complex computational processes. Within the analogy of your current PC, marbec-gpu equates to: your main processor (CPU), your graphics processor (GPU), general RAM and video RAM. Of course, with these simplifications we are leaving out some important details that we will explain in depth as we need to.\n\n\nThis will depend on what we need to do. If we just want to take a quick look at the files and review aspects of our account, we just open a browser window and go to the address https://marbec-data.ird.fr/. This will open a login interface where we just need to enter our credentials (provided by the marbec-gpu administrators).\n\nOnce inside, we will see a sort of desktop where we will see a couple of icons to access our shared directories and general documentation on the use of the platform.\n\n\n\n\nWe will start by clicking on the user options icon (the one that looks like a little person) at the top right of the desktop and selecting the Personal option.\n\nA small window will open where in the first tab shown (Account), we will have access to Change password option. Likewise, in the Display Preferences tab, we will be able to change aspects such as the interface language or the desktop image and colors.\n\n\n\n\nFrom the same Personal window seen in the previous section, in the Quota tab we will be able to verify the storage limit assigned to our user and what has been used so far in each of the folders associated to our user. This is a simple and graphic way to visualize the available space we have left. If at any time we need more space, just request it by e-mail to the marbec-data administrators.\n\n\n\n\n\n\n\nImportant\n\n\n\nIf at any time during the execution of a process the allocated quota limit is reached, the system will block any attempt to save files and this will result in the unplanned termination of the process or errors related to disk write problems.\n\n\n\n\n\nWe have a post where we develop this point in more detail.\n\n\n\n\n\n\nImportant\n\n\n\nIt is very important to define strong passwords (alphanumeric with symbols and uppercase-case) and preferably different passwords for the login in marbec-data and marbec-gpu. On the other hand, the JupyterLab environment DOES allow the use of classic shortcuts like Ctrl+C-Ctrl+V (or Cmd+C-Cmd+V in MacOS) to copy-paste character strings, so it is possible to use them during the password change process with the passwd command.",
    "crumbs": [
      "Useful links",
      "Servers",
      "Marbec DATA"
    ]
  },
  {
    "objectID": "pages/serveurs/marbec_data/index_marbec_data.html#how-to-access-marbec-data",
    "href": "pages/serveurs/marbec_data/index_marbec_data.html#how-to-access-marbec-data",
    "title": "Introduction to marbec-data",
    "section": "",
    "text": "This will depend on what we need to do. If we just want to take a quick look at the files and review aspects of our account, we just open a browser window and go to the address https://marbec-data.ird.fr/. This will open a login interface where we just need to enter our credentials (provided by the marbec-gpu administrators).\n\nOnce inside, we will see a sort of desktop where we will see a couple of icons to access our shared directories and general documentation on the use of the platform.",
    "crumbs": [
      "Useful links",
      "Servers",
      "Marbec DATA"
    ]
  },
  {
    "objectID": "pages/serveurs/marbec_data/index_marbec_data.html#how-to-change-our-password-in-marbec-data",
    "href": "pages/serveurs/marbec_data/index_marbec_data.html#how-to-change-our-password-in-marbec-data",
    "title": "Introduction to marbec-data",
    "section": "",
    "text": "We will start by clicking on the user options icon (the one that looks like a little person) at the top right of the desktop and selecting the Personal option.\n\nA small window will open where in the first tab shown (Account), we will have access to Change password option. Likewise, in the Display Preferences tab, we will be able to change aspects such as the interface language or the desktop image and colors.",
    "crumbs": [
      "Useful links",
      "Servers",
      "Marbec DATA"
    ]
  },
  {
    "objectID": "pages/serveurs/marbec_data/index_marbec_data.html#check-our-available-space-in-marbec-data.",
    "href": "pages/serveurs/marbec_data/index_marbec_data.html#check-our-available-space-in-marbec-data.",
    "title": "Introduction to marbec-data",
    "section": "",
    "text": "From the same Personal window seen in the previous section, in the Quota tab we will be able to verify the storage limit assigned to our user and what has been used so far in each of the folders associated to our user. This is a simple and graphic way to visualize the available space we have left. If at any time we need more space, just request it by e-mail to the marbec-data administrators.\n\n\n\n\n\n\n\nImportant\n\n\n\nIf at any time during the execution of a process the allocated quota limit is reached, the system will block any attempt to save files and this will result in the unplanned termination of the process or errors related to disk write problems.",
    "crumbs": [
      "Useful links",
      "Servers",
      "Marbec DATA"
    ]
  },
  {
    "objectID": "pages/serveurs/marbec_data/index_marbec_data.html#how-to-manage-files-inside-marbec-data-or-between-marbec-data-and-our-pc",
    "href": "pages/serveurs/marbec_data/index_marbec_data.html#how-to-manage-files-inside-marbec-data-or-between-marbec-data-and-our-pc",
    "title": "Introduction to marbec-data",
    "section": "",
    "text": "We have a post where we develop this point in more detail.\n\n\n\n\n\n\nImportant\n\n\n\nIt is very important to define strong passwords (alphanumeric with symbols and uppercase-case) and preferably different passwords for the login in marbec-data and marbec-gpu. On the other hand, the JupyterLab environment DOES allow the use of classic shortcuts like Ctrl+C-Ctrl+V (or Cmd+C-Cmd+V in MacOS) to copy-paste character strings, so it is possible to use them during the password change process with the passwd command.",
    "crumbs": [
      "Useful links",
      "Servers",
      "Marbec DATA"
    ]
  },
  {
    "objectID": "pages/git/miroir_github_git.html",
    "href": "pages/git/miroir_github_git.html",
    "title": "Digital Ecology Scheme (DEN) Resources and Materials",
    "section": "",
    "text": "Once you start working in a team on pieces of code, or even on informatics developments in general, the use of a version control system quickly becomes an essential tool and a precious ally for anyone wishing to manage their work efficiently. Here, we won’t go into the features of a git, or version control system, but we will propose a solution that may help to resolve a question that is often asked, which system or forge, to turn to.\nA quick look on the internet will show you that there are several forges. One of the most popular is GitHub, but others include GitLab and Bitbucket. It is also quite possible for your institute or organization to use one of these systems to host its own server (take a look here). Each system has its advantages and disadvantages, and your choice should be guided by your needs. As an example, you’ll find a quick comparison of the main forges in the table below.\n\n\n\n\n\n\n\n\n\n\nCriteria\nGitHub\nGitLab\nBitbucket\nGitea\n\n\n\n\nPopularity\nVery high\nHigh\nMedium\nLow\n\n\nCI/CD integrated\nGitHub Actions (simple and powerful)\nVery robust and flexible\nIntegrated, but limited\nDepends on manual integration\n\n\nOpen source\nNo\nYes\nNo\nYes\n\n\nFree Private Repository unlimited\nFree Private Repository unlimited\nFree Private Repository unlimited\nFree Private Repository unlimited\nRequires server\n\n\nSelf-hosting\nNo\nYes\nYes\n\n\n\nFocus private teams\nMedium\nStrong\nVery strong (integrated with Jira)\nAdapted",
    "crumbs": [
      "Useful links",
      "Version Control System",
      "Mirroring from Github to a Git Repository"
    ]
  },
  {
    "objectID": "pages/git/miroir_github_git.html#why-use-a-git-forge",
    "href": "pages/git/miroir_github_git.html#why-use-a-git-forge",
    "title": "Digital Ecology Scheme (DEN) Resources and Materials",
    "section": "",
    "text": "Once you start working in a team on pieces of code, or even on informatics developments in general, the use of a version control system quickly becomes an essential tool and a precious ally for anyone wishing to manage their work efficiently. Here, we won’t go into the features of a git, or version control system, but we will propose a solution that may help to resolve a question that is often asked, which system or forge, to turn to.\nA quick look on the internet will show you that there are several forges. One of the most popular is GitHub, but others include GitLab and Bitbucket. It is also quite possible for your institute or organization to use one of these systems to host its own server (take a look here). Each system has its advantages and disadvantages, and your choice should be guided by your needs. As an example, you’ll find a quick comparison of the main forges in the table below.\n\n\n\n\n\n\n\n\n\n\nCriteria\nGitHub\nGitLab\nBitbucket\nGitea\n\n\n\n\nPopularity\nVery high\nHigh\nMedium\nLow\n\n\nCI/CD integrated\nGitHub Actions (simple and powerful)\nVery robust and flexible\nIntegrated, but limited\nDepends on manual integration\n\n\nOpen source\nNo\nYes\nNo\nYes\n\n\nFree Private Repository unlimited\nFree Private Repository unlimited\nFree Private Repository unlimited\nFree Private Repository unlimited\nRequires server\n\n\nSelf-hosting\nNo\nYes\nYes\n\n\n\nFocus private teams\nMedium\nStrong\nVery strong (integrated with Jira)\nAdapted",
    "crumbs": [
      "Useful links",
      "Version Control System",
      "Mirroring from Github to a Git Repository"
    ]
  },
  {
    "objectID": "pages/git/miroir_github_git.html#why-turn-to-github",
    "href": "pages/git/miroir_github_git.html#why-turn-to-github",
    "title": "Digital Ecology Scheme (DEN) Resources and Materials",
    "section": "Why turn to GitHub?",
    "text": "Why turn to GitHub?\nThe procedure you are reading now offers you a solution for automatically copying the contents of a GitHub repository to the repository of another forge, such as GitLab. Without going into too much detail, and always keeping in mind that the selection of your forge should be in line with your needs, why did we choose to turn to GitHub?\nThe main reason is that GitHub offers by default a rich ecosystem and above all native integrations, notably via GitHub actions. These tools are highly effective allies for your developments, and greatly facilitate ongoing integration/deployment processes. Many communities, such as the R community, have already made available numerous Github actions. These can be used to automate a wide range of processes, from verifying your code to publishing documentation associated with developments/packages. What’s more, a large proportion of workflows can be centralized via GitHub, considerably reducing dependency on third-party tools.\nFurthermore, GitHub is the most popular forge in the world, with a huge user community. In practical terms, it’s very difficult for any other forge to rival GitHub in terms of referencing or visibility. In addition, numerous features, such as the Discussions section and the open posting of contributions, reinforce collaboration, and its interface is often perceived as the simplest and most intuitive among forges.\nIn addition to being widely adopted by enterprises and open source projects, GitHub’s AI component, via its utility GitHub Copilot, can be a great help in building your resources.\nFinally, the free version is already very powerful and offers the advantage of unlimited private repositories and collaboration with multiple contributors at no extra cost.",
    "crumbs": [
      "Useful links",
      "Version Control System",
      "Mirroring from Github to a Git Repository"
    ]
  },
  {
    "objectID": "pages/git/miroir_github_git.html#why-not-just-use-github",
    "href": "pages/git/miroir_github_git.html#why-not-just-use-github",
    "title": "Digital Ecology Scheme (DEN) Resources and Materials",
    "section": "Why not just use GitHub?",
    "text": "Why not just use GitHub?\nIn view of the previous section, we may well ask why not use only GitHub, which seems to be the smartest choice. Despite all these advantages, we mustn’t forget that GitHub remains the property of Microsoft and that, consequently, it is possible that a future change in Microsoft’s commercial policy could become penalizing or even incompatible with your work. Even if such a change is unlikely to be so “brutal” as to prevent you from taking the necessary measures, it may be wise to think about solutions that allow you to take advantage of the best of all worlds.\nThe aim of this procedure is to provide a solution for automatically copying an entire GitHub repository to another forge. For this tutorial, we’ll take the example of a forge GitLab hosted by IRD.",
    "crumbs": [
      "Useful links",
      "Version Control System",
      "Mirroring from Github to a Git Repository"
    ]
  },
  {
    "objectID": "pages/git/miroir_github_git.html#procedure-for-creating-a-mirror-between-two-repositories-github-to-gitlab-ird",
    "href": "pages/git/miroir_github_git.html#procedure-for-creating-a-mirror-between-two-repositories-github-to-gitlab-ird",
    "title": "Digital Ecology Scheme (DEN) Resources and Materials",
    "section": "Procedure for creating a mirror between two repositories (GitHub to GitLab IRD)",
    "text": "Procedure for creating a mirror between two repositories (GitHub to GitLab IRD)\n\nRequirements and scope of the procedure\nIn order to best follow the procedure, we recommend that you are at least familiar with the use of a git forge, ideally GitHub. In addition, we’ll assume that you have a properly configured account on GitHub and the destination forge (in this example, the IRD GitLab). If necessary, you’ll find the e-mail address of the resource person behind this procedure at the top of the page. Don’t hesitate to contact her if you need help.\n\nTable 1: Procedure test status.\n\n\nOperating system\nFunctional procedure\nEdition and version\n\n\n\n\nWindows\nYes\n11 Professional, version 23H2\n\n\nMac\nUntested\n\n\n\nLinux\nUntested\n\n\n\n\n\n\n1. Repository initialization\n\n1.1 Creating a source repository on GitHub\nFirst we’ll create a source repository on the GitHub forge. For the example here we’ve created a public repository named “my_github_repository” with initial settings configured globally to host R code (figure 1).\n\n\n\n\nFigure 1: Setting up the GitHub source repository\n\n\n\nFor your information, our test here creates a public repository, because we’ve taken the view that the content we’re developing is intended to be shared with everyone, and is in no way confidential or private. You can also apply this procedure to a private repository (to be tested, perhaps some parameters related to the secrets mentioned below will have to be modified), but in this case it’s a good idea to think about the veracity of using GitHub for data of a private nature. This is not to say that you should never publish private directories on GitHub, but that you should not forget that, for all its benefits, GitHub remains a Microsoft proprietary forge.\n\n\n1.2 Creating a target repository on another forge\nThe second step is to create a second target repository on another forge. As mentioned above, we’re going to use a forge GitLab hosted by IRD, to which UMR people can have access. Figure 2 below shows an example configuration.\n\n\n\n\nFigure 2: Setting up the GitLab target repository\n\n\n\nIn contrast to our GitHub source repository configuration, our repository here is published as private. This is motivated by the need to minimize the “interactions” that users can have with this repository. You’ll see later that we’re going to automate the flow of data between our source repository (GitHub) and our target repository (IRD’s GitLab), and in the end you won’t be interacting directly on the target repository, as we will on our source repository during its lifetime. Worse still, the data flow we’re going to create will be a unidirectional flow towards our source repository, so direct modifications on this repository will surely not be saved, would risk disturbing the automation and would even be contrary to the logic of the procedure.\n\n\n\n2. Linking the two forges\nNow that we’ve created the two repositories, we need to establish a connection between them. There are several ways of doing this, but what we’re going to use here is called creating an access token. Some of you may already have performed this action, especially if you’ve made a connection between a git and Rstudio. We won’t go into the details of how to create an access token, but a more detailed procedure can be carried out if required. Just to summarize, we’re going to create an access token in the target repository’s forge, which we’ll then fill in at source repository level.\n\n2.1 Creating an access token on the target repository\nTo do this, simply go to the root of our target repository (in this case, the one we’ve named “my_gitlabird_repository”). In the left-hand menu you should see a “Settings” section and an “Access tokens” sub-section. This should take you to the tab shown in figure 3.\n\n\n\n\nFigure 3: Page “Access tokens”\n\n\n\nTo create a new access token, simply click on the “Add new token” tab. In the new window, you’ll find several tabs to fill in:\n\n“Token name”, the name of the access token. Ideally, the name should be self-explanatory, allowing you to understand what it’s for. Most of us won’t have more than one access token per repository, but it is possible to add several, in which case you need to be able to identify them.\n“Expiration date”: this is the expiry date of the access token. From a security point of view, it can be dangerous to create a token that doesn’t have an expiry date (if you click on the cross to the right of the date). Beyond the simplicity of doing this (you no longer have to worry about your connection), creating a connection via an access token will create a potential vulnerability in the security of your repository, which could serve as an entry point for potential malicious attacks. There’s no need to become paranoid, but the idea is more to think about the lifespan of your token. Is my project short-term? Is there a future deadline that is likely to alter the relevance of this token (for example, a change in the integrity of the source repository)? In concrete terms, am I going to think about deleting my access token if I no longer need it? You’re free to set your own rules. Here, for example, we’ve identified a token that will be valid until 01/05/2025.\n“Select a role”. In the case of using a personal access token to mirror actions from GitHub to GitLab, we don’t need to focus directly on roles, as the scopes in the next section are what determine the token’s permissions. However, the role associated with a personal access token may influence certain project or group access permissions. If you want to be strict, it’s best to choose a role such as “Developer”. A developer is an entity that can push code, create branches, make pull requests and manage repositories (which is what we want to do here).\n“Selected scopes”. This last section is the one that will define permissions and concretely what we can access with our token. To make a mirror, we need 3 specific rights:\n\n“api”: allows you to perform all API actions, including managing repositories, projects, etc.\nwrite_repository”: allows you to push in GitLab repositories (necessary for mirroring).\nread_repository”: allows you to read GitLab repositories (if required for configuration or verification).\n\n\n\n\n\n\nFigure 4: Access token configuration\n\n\n\nValidate your configuration through the “Create project access token” button at the bottom.\nThe next page should show you that your access token has been validated, as well as its value. You can view it by clicking on the eye-shaped button. Just below your token, you’ll notice a message telling you that this token will only be revealed now, and it will no longer be possible to view it afterwards (for security reasons). The idea is to copy it (click on the button to the right of the eye-shaped one), store it somewhere (in a password manager, for example) and then upload it to our GitHub a source.\n\n\n2.2 Setting our access token on the source repository\nNow that we have our access token for our target repository, we’ll need to fill it in for our source repository. To do this, we need to go to the page for our source repository (in the example, the GitHub repository we’ve named “my_github_repository”), click on the “Settings” tab, the “Secrets and variables” section and the “Actions” sub-section. On the new page that appears, click on the “New respository secret” button in the “Repository secrets” section. All you then need to do is enter a name for this secret (as before, it must be meaningful to the user) and paste the value of your token in the “Secret” section (figure 5).\n\n\n\n\nFigure 5: Configuring a secret associated with a GitHub repository\n\n\n\n\n\n\n3. Creating and automating the mirroring process\n\n3.1 Creating the GitHub action script\nWith our two repositories connected, we can now start work on creating the mirroring process and automating it. To do this, we’re going to create a GitHub action. We’ve talked about this before, but this type of process will enable us to run processes in the background and, above all, to automate their launch.\nTo do this, we have two options: (1) manually create and adapt our yaml file associated with the “GitHub Action” or (2) use a function in the package sparck which will simplify the creation process.\n\n3.1.1 Creating and manually adapting the GitHub Action\nTo do this, we need to go to the root of our GitHub source repository and create a “.github” folder and a “workflows” subfolder. Inside the latter folder, we’ll copy the code below into a source code editor (such as Notepad or Visual Studio Code).\nname: GitHub to GitLab IRD mirror with release assets\n\non:\n  push: \n    branches:\n      - '**'\n    tags:\n      - '**'\n  pull_request:\n    branches:\n      - '**'\n  delete:\n    branches:\n      - '**'\n    tags:\n      - '**'\n  release:\n    types:\n      - created\n      - published\n      - edited\n      - deleted\n\njobs:\n  mirror:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Clone repository as bare\n        run: |\n          git clone --bare &lt;github_repository_source_url&gt;.git my-github-repository.git\n\n      - name: Set up Git\n        run: |\n          git config --global user.name \"GitHub Actions\"\n          git config --global user.email \"github-actions@users.noreply.github.com\"\n\n      - name: Add forge remote\n        run: |\n          cd my-github-repository.git\n          git remote add mirror https://oauth2:${{ secrets.&lt;secret_token_name&gt; }}@&lt;git_repository_target_url&gt;.git\n  \n      - name: Push to forge\n        run: |\n          cd my-github-repository.git\n          git push --mirror mirror\n\n  download-release-assets:\n      runs-on: ubuntu-latest\n      needs: mirror\n\n      steps:\n        - name: Set up Git (Authentication)\n          run: |\n            git config --global user.name \"GitHub Actions\"\n            git config --global user.email \"github-actions@users.noreply.github.com\"\n\n        - name: Fetch release(s) from GitHub\n          id: fetch_releases\n          run: |\n            RESPONSE=$(curl -s -H \"Authorization: token ${{ secrets.GITHUB_TOKEN }}\" \\\n              \"https://api.github.com/repos/&lt;github_repository_source_url_api&gt;/releases\")\n          \n            RELEASE_IDS_NAMES=$(echo \"$RESPONSE\" | jq -r '.[] | \"\\(.id) \\(.name)\"')\n\n            if [ -z \"$RELEASE_IDS_NAMES\" ]; then\n              echo \"No release found. No action required.\"\n              echo \"SKIP_NEXT_STEP=true\" &gt;&gt; $GITHUB_ENV\n              exit 0\n            fi\n\n            NUM_RELEASES=$(echo \"$RELEASE_IDS_NAMES\" | wc -l)\n            echo \"Number of releases found: $NUM_RELEASES\"\n            echo \"NUM_RELEASES=$NUM_RELEASES\" &gt;&gt; $GITHUB_ENV\n\n            RELEASE_IDS=\"\"\n            RELEASE_NAMES=\"\"\n            \n            while IFS= read -r line; do\n              RELEASE_ID=$(echo \"$line\" | awk '{print $1}')\n              RELEASE_NAME=$(echo \"$line\" | awk '{print $2}')\n              RELEASE_IDS=\"$RELEASE_IDS$RELEASE_ID,\"\n              RELEASE_NAMES=\"$RELEASE_NAMES$RELEASE_NAME,\"\n            done &lt;&lt;&lt; \"$RELEASE_IDS_NAMES\"\n\n            RELEASE_IDS=${RELEASE_IDS%,}\n            RELEASE_NAMES=${RELEASE_NAMES%,}\n\n            echo \"RELEASE_IDS=$RELEASE_IDS\" &gt;&gt; $GITHUB_ENV\n            echo \"RELEASE_NAMES=$RELEASE_NAMES\" &gt;&gt; $GITHUB_ENV\n\n        - name: Download release(s) asset(s) from GitHub\n          id: download_assets\n          if: ${{ env.SKIP_NEXT_STEP != 'true' }}\n          run: |\n            ASSETS_FOUND=false\n            NUM_RELEASES=${{ env.NUM_RELEASES }}\n            RELEASE_IDS=${{ env.RELEASE_IDS }}\n            RELEASE_NAMES=${{ env.RELEASE_NAMES }}\n            IFS=',' read -ra RELEASE_IDS_ARRAY &lt;&lt;&lt; \"$RELEASE_IDS\"\n            IFS=',' read -ra RELEASE_NAMES_ARRAY &lt;&lt;&lt; \"$RELEASE_NAMES\"\n            for num_release in $(seq 0 $((NUM_RELEASES - 1))); do\n              RELEASE_ID=\"${RELEASE_IDS_ARRAY[$num_release]}\"\n              RELEASE_NAME=\"${RELEASE_NAMES_ARRAY[$num_release]}\"\n              echo \"Processing release ID: $RELEASE_ID with Name: $RELEASE_NAME\"\n              ASSETS=$(curl -s \\\n                -H \"Authorization: token ${{ secrets.GITHUB_TOKEN }}\" \\\n                \"https://api.github.com/repos/&lt;github_repository_source_url_api&gt;/releases/$RELEASE_ID/assets\" \\\n                | jq -r '.[].browser_download_url')\n              if [ -z \"$ASSETS\" ]; then\n                echo \"No assets found for release $RELEASE_ID ($RELEASE_NAME). Skipping download step.\"\n                continue\n              else\n                ASSETS_FOUND=true\n                mkdir -p \"release-assets/$RELEASE_ID\"_\"$RELEASE_NAME\"\n                cd \"release-assets/$RELEASE_ID\"_\"$RELEASE_NAME\"\n        \n                for URL in $ASSETS; do\n                  echo \"Downloading $URL\"\n                  curl -L -o \"$(basename \"$URL\")\" -H \"Authorization: token ${{ secrets.GITHUB_TOKEN }}\" \"$URL\"\n                done\n        \n                cd -\n              fi\n            done\n            if [ \"$ASSETS_FOUND\" = false ]; then\n              echo \"No assets found for any release. Exiting.\"\n              echo \"SKIP_NEXT_STEP=true\" &gt;&gt; $GITHUB_ENV\n              exit 0\n            fi\n\n        - name: Push asset(s) to mirror repository\n          id: push_mirror\n          if: ${{ env.SKIP_NEXT_STEP != 'true' }}\n          run: |\n            git clone https://oauth2:${{ secrets.&lt;secret_token_name&gt; }}@&lt;git_repository_target_url&gt;.git\n            cd test_miroir_github\n\n            if [ -d \"release-assets\" ]; then\n              echo \"Removing existing release-assets directory from the mirror repository.\"\n              rm -rf release-assets\n            fi\n\n            echo \"Copying local release-assets directory to the mirror repository.\"\n            cp -r \"../release-assets\" .\n\n            git add .\n            git commit -m \"Add release assets from GitHub releases\"\n\n            BRANCH_NAME=$(git symbolic-ref --short HEAD)\n\n            git push origin \"$BRANCH_NAME\"\nIn this script, you’ll need to adapt certain variables to suit your environment:\n\n&lt;github_repository_source_url&gt; for the URL address of your GitHub source directory. In this example, the value is “https://github.com/umr-marbec/my_github_repository” (without quotation marks, the same applies to all subsequent variables).\n&lt;secret_token_name&gt; which will fill in the name of the secret we’ve identified in the GitHub source repository (see section 2.2). Here we’ll use “TOKEN_MY_GITLABIRD_REPOSITORY”. If you notice in figure 5, the name of my secret was in lower case. By default, GitHub switches all characters to uppercase.\n&lt;git_repository_target_url&gt; for the ULR address of your target repository, without the “https://” value at the start of the string. For example, here for the IRD forge we’ll use the value “forge.ird.fr/marbec/private/depetris-mathieu/my_gitlabird_repository”.\n&lt;github_repository_source_url_api&gt; for the URD address of the GitHub source directory, but in a “light” version (without “https://github.com/”). For our example, the value will be “umr-marbec/my_github_repository”.\n\nOnce you’ve correctly replaced these variables, all you have to do is save the file in YAML format (use the .yml extension to save it). For our example here, my file will be called mirror_github_to_irdgitlab.yml and it will be placed as indicated above in the “workflows” directory we’re creating on the target GitHub repository.\n\n\n3.1.2 Using the add_github_action() function in the sparck R package\nIf you prefer a simplified approach, you can use the R package sparck and the associated function add_github_action().\nTo do this, start by installing it under R with the following command:\n# You'll need the devtools package to download the sparck package from his GitHub repository.\n# If necessary, use install.packages(“devtools”)\ndevtools::install_github(\"https://github.com/umr-marbec/sparck\")\nlibrary(sparck)\nNext, you need to define the R working directory as your repository. If you’re using your repository for R-related code, you may have an .Rproj file in it that allows you to launch an R session directly in the repository.\nThe last step is to call the add_github_action() function with the configuration parameters for your environment. If we take our example for this procedure, the command line will be as follows:\nadd_github_action(github_action_name = \"mirror_github_git\",\n                  arguments = c(\"github_repository_source_url\" = \"https://github.com/umr-marbec/my_github_repository\",\n                                \"secret_token_name\" = \"TOKEN_MY_GITLABIRD_REPOSITORY\",\n                                \"git_repository_target_url\" = \"https://forge.ird.fr/marbec/private/depetris-mathieu/my_gitlabird_repository\"))\nCompared with a manual modification of our “GitHub Action” (step in 3.1.1), you’ll notice that the variables to be filled in are much “simpler” and that the function automatically takes care of formatting and creating the “.github” folder and “workflows” subfolder in your working directory (you’ll need to have the associated rights to modify your file system).\n\n\n\n3.2 Configuring the main branch of the target repository\nAt this stage of the procedure, your GitHub action should be functional. However, if it launches (for example, by making a modification, such as a push, on our GitHub source repository) you should get an error like the one shown in figure 6.\n\n\n\n\nFigure 6: Error related to a protected branch\n\n\n\nThis error indicates that the branch in our target repository is protected and won’t allow our process to synchronize. To resolve this problem, simply go to the target repository, as before, to the “Settings” tab, “Repository” section and “Protected branches” sub-section (figure 7).\n\n\n\n\nFigure 7: Page “Protected branches”\n\n\n\nIn general, default git branches are often protected. This allows most users to automatically apply security measures to avoid inadvertently carrying out actions that could affect the integrity of the repository. In our case, we know what we want to do and we need to lift these protections in order to perform our mirror. To do this, simply click on the red “Unprotect” button on your default branch (at this stage you should only have one) and the action in the popup window that appears.\n\n\n\n4. A final word\nCongratulations, if you’ve reached this point, you should have a working mirror between your two repositories, which launches its associated process for each modification on the source repository.\nA few tips for the future:\n\nDon’t hesitate to give us feedback on this procedure, especially if you have any suggestions for improvement. For example, testing this procedure on several operating systems, or with specifications other than those outlined here (such as testing on a private source repository) would be very enriching feedback.\nDuring ad hoc testing, we noted a number of failures in the mirroring process. In concrete terms, your “ordinary” actions linked to code integration, branch creation or most basic actions did not show any failures. On the other hand, when you start to perform realeases, add assets to them, play a little with the limits by deleting them and the associated tags …., sometimes the mirror doesn’t trigger. Normally this is quickly rectified by the next mirror on a commit, for example, but please don’t hesitate to report failures so that we can improve the procedure.",
    "crumbs": [
      "Useful links",
      "Version Control System",
      "Mirroring from Github to a Git Repository"
    ]
  },
  {
    "objectID": "pages/contribution.html",
    "href": "pages/contribution.html",
    "title": "Digital Ecology Scheme (DEN) Resources and Materials",
    "section": "",
    "text": "Do you belong to the UMR, or not, and have some pieces of code, technical documentation or any other informatics resources that you’d like to test, standardize or even share? Do you have an idea and wonder whether the process has already been developed by someone else? Then you’ve come to the right place!\nThis site, and by extension the associated resources, are the result of several hours’ work and joint reflection on subjects which, in the final analysis, are often transverse between people. Its aim is to centralize and standardize as much as possible what has been done, and its success depends entirely on the synergy we bring to creating content. In fact, these resources are just the tip of the iceberg in terms of the relationships and links we can forge with each other on our work.\nIf you’d like to contribute in some way, the key is not to be discouraged by the size of the task, which can be frightening, but to start by taking the first step. You’ll quickly realize that behind any problem or obstacle, you’ll find a rich and friendly community that’ll be able to help you, always with the aim of sharing and optimizing informatics resources.\nBehind the many different types of content we can share with each other, from markdown documentation to packages or software developed by the community, the idea is to establish standards between us that guarantee the integrity and interoperability of the content and resources available. These standards can be seen as rules, but in no way should they be assimilated to barriers to your involvement. For example, if you’re stuck on specific points (such as translating a procedure into another language or not knowing how to use forges such as git), turn to the community and you’ll find all the help you need.\nIn general, here are a few guidelines to be considered before publishing resources:\n\nthe resources on this site are intended to be used by all UMR, but also potentially by associated partners. By default, content must be published in French, but also in English to ensure maximum accessibility. Some content, such as training courses, may be exempted from these rules if this is relevant. Once again, don’t let the language barrier stop you - you’re guaranteed to find someone in our community who can help you translate.\ndepending on the type of content you want to publish, it’s sometimes best to turn to resource persons or referents who can provide you with solutions or proposals to guide you in sharing your resources. In the meantime, if you don’t know who to turn to, you can send an e-mail to DEN administrators.\ntemplates will be proposed for the different types of resources, with a view to sharing and overall harmony. In the meantime, don’t hesitate to get in touch with other members of the community to try and find a common structure wherever possible.\neven if the use of informatics resources within the UMR is really to be seen in a transversal way and without associated “borders”, the mutualization and accessibility of the resources is really in connection with the Digital Ecology Device, or DEN. This entity, which is transversal to the UMR, aims to set up, coordinate and pool technical resources, as well as exchange methodologies and new approaches in support of the digital aspects of scientific research. Getting closer to this organization and its sub-entities could be a wise move, and provide you with significant support for your activities.\nBy its very essence, this site is designed to evolve as needs change. Don’t hesitate to suggest improvements, new sections or even a different organization. In the end, this will only benefit the community.\n\n\n\n\n Back to top",
    "crumbs": [
      "Useful links",
      "I want to contribute!"
    ]
  },
  {
    "objectID": "pages/packages_logiciels/r_package_sparck.html",
    "href": "pages/packages_logiciels/r_package_sparck.html",
    "title": "R package sparck",
    "section": "",
    "text": "The sparck package has been developed for the R software. Its aim is to provide standardized functions and processes to support work of the UMR and, by extension, its partners. The associated functions cover topics such as data manipulation, data analysis and work environment configuration. Far from the idea of covering all themes or subjects exhaustively, its vocation is really to provide a standard in terms of development in order to improve the transversality of actions and improve the associated links.\nIn relation with the “I want to contribute!” section, take a look at the general documentation, as well as the issues section of the GitHub repository.",
    "crumbs": [
      "Useful links",
      "Packages and software",
      "R package sparck"
    ]
  },
  {
    "objectID": "pages/packages_logiciels/r_package_sparck.html#support-package-for-analysis-research-collaboration-and-knowledge-1",
    "href": "pages/packages_logiciels/r_package_sparck.html#support-package-for-analysis-research-collaboration-and-knowledge-1",
    "title": "R package sparck",
    "section": "",
    "text": "The sparck package has been developed for the R software. Its aim is to provide standardized functions and processes to support work of the UMR and, by extension, its partners. The associated functions cover topics such as data manipulation, data analysis and work environment configuration. Far from the idea of covering all themes or subjects exhaustively, its vocation is really to provide a standard in terms of development in order to improve the transversality of actions and improve the associated links.\nIn relation with the “I want to contribute!” section, take a look at the general documentation, as well as the issues section of the GitHub repository.",
    "crumbs": [
      "Useful links",
      "Packages and software",
      "R package sparck"
    ]
  },
  {
    "objectID": "pages/packages_logiciels/ichthyop.html",
    "href": "pages/packages_logiciels/ichthyop.html",
    "title": "Ichthyop software",
    "section": "",
    "text": "Ichthyop is a free Java tool designed to study the effects of physical and biological factors on ichthyoplankton dynamics.\nIt incorporates the most important processes involved in fish early life: spawning, movement, growth, mortality and recruitment. The tool uses as input time series of velocity, temperature and salinity fields archived from ROMS, MARS, NEMO or SYMPHONIE oceanic models (either files or OpenDAP).\nThe Ichthyop software and its associted ressources are available on GitHub.",
    "crumbs": [
      "Useful links",
      "Packages and software",
      "Ichthyop software"
    ]
  },
  {
    "objectID": "pages/packages_logiciels/ichthyop.html#lagrangian-tool-for-simulating-ichthyoplankton-dynamics",
    "href": "pages/packages_logiciels/ichthyop.html#lagrangian-tool-for-simulating-ichthyoplankton-dynamics",
    "title": "Ichthyop software",
    "section": "",
    "text": "Ichthyop is a free Java tool designed to study the effects of physical and biological factors on ichthyoplankton dynamics.\nIt incorporates the most important processes involved in fish early life: spawning, movement, growth, mortality and recruitment. The tool uses as input time series of velocity, temperature and salinity fields archived from ROMS, MARS, NEMO or SYMPHONIE oceanic models (either files or OpenDAP).\nThe Ichthyop software and its associted ressources are available on GitHub.",
    "crumbs": [
      "Useful links",
      "Packages and software",
      "Ichthyop software"
    ]
  },
  {
    "objectID": "pages/support/gestionnaire_mots_de_passe.html",
    "href": "pages/support/gestionnaire_mots_de_passe.html",
    "title": "Digital Ecology Scheme (DEN) Resources and Materials",
    "section": "",
    "text": "Authentication of users, from simple users to administrators, represents an important element in managing the security of an information system. In practice, access to many digital services depends on the use of passwords, which remain one of the simplest and least costly means of controlling access and eventually proving identity. However, managing or defining these passwords can become laborious, and can even lead to system vulnerabilities. By way of example, a Verizon study in 2021 estimates that 81% of global data breach notifications are linked to password issues. Even worse, in France around 60% of the notifications received by there CNIL since the beginning of 2021 are related to hacking, and a large number could have been avoided by following good password practices.\nThe aim of this document is not to go into detail on the recommendations to be followed in order to develop the “best” authentication measures, but rather to provide you with a ready-to-use solution for setting up a password manager and, above all, to raise your understanding of how to use it.\nFurthermore, we’ll take a brief look at some best-practice authentication strategies, but if you’d like to find out more, it’s highly advisable to turn to the ANSSI and CNIL resources on the subject. Below, for example, you’ll find two resources that are accessible in terms of content and easy to understand, particularly with regard to the associated applications:\n\nANSSI, Recommendations for multi-factor authentication and passwords,\nCNIL, Passwords: a new recommendation to control security.",
    "crumbs": [
      "Useful links",
      "Global support",
      "Password manager"
    ]
  },
  {
    "objectID": "pages/support/gestionnaire_mots_de_passe.html#access-authentication-a-key-element-in-information-system-security-management",
    "href": "pages/support/gestionnaire_mots_de_passe.html#access-authentication-a-key-element-in-information-system-security-management",
    "title": "Digital Ecology Scheme (DEN) Resources and Materials",
    "section": "",
    "text": "Authentication of users, from simple users to administrators, represents an important element in managing the security of an information system. In practice, access to many digital services depends on the use of passwords, which remain one of the simplest and least costly means of controlling access and eventually proving identity. However, managing or defining these passwords can become laborious, and can even lead to system vulnerabilities. By way of example, a Verizon study in 2021 estimates that 81% of global data breach notifications are linked to password issues. Even worse, in France around 60% of the notifications received by there CNIL since the beginning of 2021 are related to hacking, and a large number could have been avoided by following good password practices.\nThe aim of this document is not to go into detail on the recommendations to be followed in order to develop the “best” authentication measures, but rather to provide you with a ready-to-use solution for setting up a password manager and, above all, to raise your understanding of how to use it.\nFurthermore, we’ll take a brief look at some best-practice authentication strategies, but if you’d like to find out more, it’s highly advisable to turn to the ANSSI and CNIL resources on the subject. Below, for example, you’ll find two resources that are accessible in terms of content and easy to understand, particularly with regard to the associated applications:\n\nANSSI, Recommendations for multi-factor authentication and passwords,\nCNIL, Passwords: a new recommendation to control security.",
    "crumbs": [
      "Useful links",
      "Global support",
      "Password manager"
    ]
  },
  {
    "objectID": "pages/support/gestionnaire_mots_de_passe.html#what-is-a-password-manager-why-is-it-useful",
    "href": "pages/support/gestionnaire_mots_de_passe.html#what-is-a-password-manager-why-is-it-useful",
    "title": "Digital Ecology Scheme (DEN) Resources and Materials",
    "section": "What is a password manager? Why is it useful?",
    "text": "What is a password manager? Why is it useful?\nA password manager, also known as a password vault, is a virtual storage facility for login credentials, where all information is encrypted. In addition to pure storage, these solutions often enable you to automatically fill in login details for websites, or synchronize information on several devices at the same time, for example between two computers or even with smartphones. In addition, password creation is often facilitated by organizing passwords into groups and sub-groups, or by generating passwords to comply with associated recommendations and best practices. In practice, this often takes the form of a strong, human-memorized “master” password, which unlocks the entire password database (and thus access to associated resources). An alternative to this system, for example, is to use a password-protected office file. This solution is of course not to be preferred, as it does not provide the same level of protection as a password safe designed for this purpose. Finally, the use of a password manager is one of the recommendations made by ANSSI (n°31) concerning “multi-factor authentication and passwords”.\nIn this world of digital safe software, we’re going to find paid-for solutions such as NordPass or Bitwarden, but there is a very good free and open source solution KeePass, which has the advantage of being certified by the ANSSI in its version 2.10 portable.\nIn our proposal below, we’ll be using a little sidekick to KeePass, called KeePassXC, which is based on the KeePass architecture, but with the addition of the multi-platform component. More specifically, there’s nothing fundamentally wrong with using KeePass, however, it’s developed in C# and therefore requires Microsoft’s .NET platform, whereas KeePassXC is developed in C++ and runs natively on Linux, macOS and Windows, enabling the best possible integration.\nTo conclude this section, and above all to give some of our more advanced users a deeper insight into the choice of:\n\nKeePassXC does not support KeePass plugins. This isn’t necessarily a bad thing, as plugins can be dangerous, and KeePassXC already provides many features that require third-party plugins in KeePass, so for most things you should never need them.\nAs a little bonus, even though it’s security that’s important, KeePassXC’s interface looks great and is more pleasant to use than KeePass.\nKeePassXC is natively available in French.\nIf you want to see for yourself, the KeePass database file is perfectly compatible with KeePassXC (and vice versa). So you can switch from one system to the other without losing your data.",
    "crumbs": [
      "Useful links",
      "Global support",
      "Password manager"
    ]
  },
  {
    "objectID": "pages/support/gestionnaire_mots_de_passe.html#keepassxc-setup-and-configuration",
    "href": "pages/support/gestionnaire_mots_de_passe.html#keepassxc-setup-and-configuration",
    "title": "Digital Ecology Scheme (DEN) Resources and Materials",
    "section": "1 - KeePassXC setup and configuration",
    "text": "1 - KeePassXC setup and configuration\n\n1.1 - Requirements and framework of the procedure\nFor this procedure we will have the following objectives:\n\ninstalling and configuring KeePassXC correctly\nquick overview of how to use the software\ninstall and configure an extension to link our password manager to my browser for semi-automatic filling of website passwords\nset up synchronization between two computers and an Android smartphone\ninstallation of an Android KeePass application\n\nIMPORTANT: setting up one or more synchronization processes should not be considered as a backup procedure, and should never replace a dedicated backup of your database with your personal data. For example, if a problem occurs with a database on one device, synchronization with other devices will propagate the problem.\nIf you need help, you’ll also find a comprehensive documentation package at link.\n\n\n\nOperating system\nFunctional procedure\nEdition and version\n\n\n\n\nWindows\nYes\n11 Professional, version 24H2\n\n\nMac\nUntested\n\n\n\nLinux\nYes\nUbuntu 24.04.2 LTS\n\n\nAndroid\nYes\nVersion 13 & 15\n\n\niOS\nUntested\n\n\n\n\nTable 1: Procedure test status.\n\n\n1.2 - Downloading and installing KeePassXC\n\n1.2.1 - For Microsoft Windows\nGo to the download page on the official website via the following link and download the msi archive. Follow the usual installation steps, making sure you run the installation in administrator mode (if necessary, right-click on the executable and click on “Run as administrator”). If you’ve done this correctly, you should get a window like the one in figure 1. You can check the “Create a shortcut on the desktop” box, which will place a launch shortcut on your desktop.\n\n\n\nFigure 1: Windows installation wizard\n\n\nIf the default installation path points to something like “C:\\Users\\nom_user”, you have not launched the installation with administrator rights.\nNote that for some users, the software won’t launch after installation, even if you click on the run shortcut. To remedy this, you need to install the latest Microsoft Visual C++ libraries. These can be found at next.\n\n\n1.2.2 - Linux\nSimply download the KeepPassXC AppImage for Linux from official link and install it on your system.\n\n\n1.2.3 - For macOS systems\nFor macOS systems, download the dmg image from the [official] link (https://keepassxc.org/download/#macos){.external target=“_blank”} and install the software in your applications.\n\n\n1.2.4 - Initial configuration and creation of your password database\nNow that the application is correctly installed on your system, you should come across a welcome window like the figure 2.\n\n\n\nFigure 2: Application home page\n\n\nBy default, the application language is set to the system language. In line with my current configuration, future screenshots will be in French.\nNow we’re going to create our database file, which will contain all our passwords. To do this, simply click on the “Create database” tab. The next window will prompt you to give this database a name. This database will take the form of a .kdbx file stored on our hard disk. To avoid any compatibility problems, use a name with a special character (no accent or space). Here, for the example, my database will be called “bdd_test_keepassxc” (figure 3).\n\n\n\nFigure 3: Database creation\n\n\nNo need to modify the “Encryption parameters” unless you want to customize them to suit your requirements. The next window, “Database Identifiers”, lets you define the “master” password that will protect your database (and therefore all its contents). To summarize:\n\nif you only have one password to remember, this is it!\nit’s considered the one that unlocks all the others, so it has to be complicated and comply with a minimum of safety rules. Without going into too much detail about these rules, you can click on the square icon with 3 dots to open a generation aid utility (figure 4).\nthis password is very important and its loss, as a general rule, will make it impossible to open the database and thus access your passwords (by default through password recovery processes).\nthere’s a box underneath to “Add another protection”. The available options won’t be discussed here, but you should know that you can add security and access options, such as a key file or Q&A.\n\n\n\n\nFigure 4: Defining logins for database access\n\n\nThe last step asks where you want to save your database and what name you want to give it. In my example, my database will be saved as bdd_test_keepassxc.kdbx. On the other hand, if you wish to synchronize your database between several devices (see section 4, for example, for synchronization between a computer and a smartphone), it’s best to place this file in a folder. As an example here, my .kdbx file will be stored in a file named “keepass”.\nBefore I start creating my first passwords, take a look at the application settings. To do this, simply click on the gearwheel, or click on the “Tools” tab and then on “Settings” (figure 5).\n\n\n\nFigure 5: Access to application parameters\n\n\nYou’re free to modify these settings to suit your preferences, but I’d recommend at least the following to enhance your interface and interactions. Please note that these options may differ slightly between versions (depending on the OS), particularly between Windows and Linux.\nIn the “General” tab, “Startup” section (figure 6) :\n\nif you have not already done so, check “Automatically launch KeePassXC at system startup”. Each time your system is started up, the software will launch automatically and ask you for your “master” password to unlock your database,\ncheck the “Minimize window after unlocking database” tab. Paired with the previous option, unlocking your database doesn’t require you to use a password right away. So there’s no need to keep the application front and center on your desktop.\n\n\n\n\nFigure 6: ‘General’ tab, ‘Startup’ section\n\n\nIn the “General” tab, “Entry Management” section (figure 7) :\n\ncheck “Copy data on double clicking field in entry view”.\ncheck “Minimise when opening a URL”.\n\n\n\n\nFigure 7 : Entry management Figure 7: ‘General’ tab, ‘Entry management’ section\n\n\nIn the “General” tab, “User Interface” section (figure 8) :\n\ncheck “Minimise instead of app exit”.\ncheck “Show a system tray icon”.\ncheck “Hide window to system tray when minimized”.\n\n\n\n\nFigure 8: ‘General’ tab, ‘User Interface’ section\n\n\nWith these settings, when you close or minimize the application’s window, the application moves into your notification area (it can be in the hidden area the first time, figure 9) instead of closing the application permanently (to do this, click on the “Database” tab, then “Quit” at the very bottom). When your application is in this zone, simply double-click on it to open it.\n\n\n\nFigure 9: Application in the notification area",
    "crumbs": [
      "Useful links",
      "Global support",
      "Password manager"
    ]
  },
  {
    "objectID": "pages/support/gestionnaire_mots_de_passe.html#application-overview",
    "href": "pages/support/gestionnaire_mots_de_passe.html#application-overview",
    "title": "Digital Ecology Scheme (DEN) Resources and Materials",
    "section": "2 - Application overview",
    "text": "2 - Application overview\n\n2.1 - Application interface\nYour database interface is divided into 4 parts (figure 10).\n\n\n\nFigure 10: Main database interface\n\n\nThe first section lists all your passwords. You can organize them into groups and even sub-groups (here, by default, a group named “Root” has been created) and you can even change the icons of the items.\nThe second part lists the entries present in your groups and in your database in general. As before, you can modify the display of these entries, in particular by modifying the associated icons.\nThe third section contains shortcuts to dynamic groups or tags you’ve created. These latter resources are parameters that you can associate with entries and/or groups, providing simplified access, rather like shortcuts. An interesting detail is that this section also includes tabs for diagnosing the status of some of your passwords, notably by searching for expired passwords or low levels of security (for example, if your password is very short in terms of the number of characters).\nThe last part is the preview tab, which gives you a summary of the contents of your entries, for example.\nThe toolbar is divided into 5 sections (figure 11).\n\n\n\nFigure 11: Toolbar interface\n\n\nPart 1 includes tabs for opening a database, saving the active database or locking it.\nSection 2 allows you to create or modify entries (add and/or delete).\nSection 3 allows you to use shortcuts to copy certain elements from selected entries.\nSection 4 provides access to database parameters and reports, as well as the password generator.\nThe last section provides access to a search bar.\n\n\n2.2 - Entry creation\nTo create a new entry, simply click on the + icon in section 2 of the toolbar. By default, we were in the “Root” group, so this entry will be located in this group. Here’s an example using my CNRS account. Here, the minimum information required to create an entry is “Title”, “User name” and “Password”.(figure 12). We’ll leave the “URL” field empty for the moment, and use it later to automatically populate fields via the browser. Note that you have many other options, such as the “Icon” menu on the side or the “Notes” field, which lets you add comments or hints to your entry. By default, the password is hidden. This is very useful, as you don’t need to be able to see the password to use it later (especially useful if you’re in public places).\n\n\n\nFigure 12: Entry creation\n\n\nOnce the action has been validated, the entry is found in our database (figure 13). To improve the readability of my interface, I’ve even changed the name of the “Root” group to “cnrs”.\n\n\n\nFigure 13: Input interface\n\n\nNow if I want to copy part of my entry, for example the user name or password, all I have to do is use the buttons in section 3 of the toolbar. Note that when copying a password (the button with the key), you have a limited time in which to use it before it is deleted from your “copy memory” (= clipboard), figure 14). You can configure this time in the “Settings” menu, in the “Security” tab, in the “Timeout” section and the “Clear clipboard after” option.\n\n\n\nFigure 14: Clipboard time",
    "crumbs": [
      "Useful links",
      "Global support",
      "Password manager"
    ]
  },
  {
    "objectID": "pages/support/gestionnaire_mots_de_passe.html#browser-integration",
    "href": "pages/support/gestionnaire_mots_de_passe.html#browser-integration",
    "title": "Digital Ecology Scheme (DEN) Resources and Materials",
    "section": "3 - Browser integration",
    "text": "3 - Browser integration\nOne of the most practical options is to integrate your safe with your browser. More concretely, this means you’ll be able to fill in usernames and passwords semi-automatically when you visit related websites.\nKeePassXC integrates with the following browsers:\n\nBased on Chrome, Chromium, Vivaldi and Brave,\nBased on Mozilla Firefox and Tor-Browser,\nBased on Microsoft Egde.\n\nThe first thing to do is activate the association option in the KeePassXC settings. To do this, simply go to the “Browser integration” tab in the settings. (figure 15).\n\n\n\nFigure 15: Browser integration\n\n\nFor my example, I’m going to test with the Egde browser, but the principle remains the same for the others, and you can even combine several at once.\nThe second step is to install the extension on your browser. Click on the links just below the “Browser integration” tab. You should be redirected to an installation page (again, here with the Microsoft Egde example, figure 16).\n\n\n\nFigure 16: Browser extension installation\n\n\nAfter installation, restart your browser and you should have the extension in this form (figure 17, the KeePassXC database must be unlocked).\n\n\n\nFigure 17: Browser extension configuration\n\n\nNow click on the “Connect” button, then on your database window specify a name for the key association request (figure 18).\n\n\n\nFigure 18: Browser extension key association\n\n\nYour extension should now be correctly configured (figure 19).\n\n\n\nFigure 19: Complete browser extension configuration\n\n\nNext, we’re going to test semi-automatic filling on the entry I created earlier in connection with my CNRS login. To do this, I go to a CNRS login page (figure 20).\n\n\n\nFigure 20: CNRS login page\n\n\nHere I’m going to copy the URL of this page in order to add it to my database entry. You’ll notice that the extension has already detected the possibility of adding fields linked to an entry in my database (via the extension’s greyed-out tab). What’s more, you don’t need to copy the entire URL, just stop after the .fr.You can apply this rule to other URLs, as it allows you to generalize auto-fill detection, although there are rare cases where it doesn’t work. In our case, we’ll just use the URL “https://janus.cnrs.fr/”, which we’ll add to the “URL” field of our previous entry. (figure 21).\n\n\n\nFigure 21: Adding a URL to our entry\n\n\nNow refresh your browser page (being on the CNRS login page) or click on the extension icon in the browser, then click on “Redetect login fields”. The following KeePassXC window should appear (figure 22).\n\n\n\nFigure 22: Accepting URL association\n\n\nAll you have to do now is check the “Remember” and “Allow selected items” boxes. Now, every time you arrive on a page with “https://janus.cnrs.fr/” at the beginning of the URL, KeePassXC will suggest semi-automatically filling in fields related to what you’ve entered in your database (figure 23).\n\n\n\nFigure 23: Automatic filling\n\n\nOne last little tip, rather than clicking on the extension icon or field to select the corresponding entry in the database, you can use keyboard shortcut functions. You’ll find them in the browser’s extension settings tab (by clicking on the green notched road), figure 24), “General” tab, “Keyboard shortcuts” section. These are configurable, and in my default example (under Windows) you use Alt + Shift + U to enter your username and password.\n\n\n\nFigure 24: Browser extension settings",
    "crumbs": [
      "Useful links",
      "Global support",
      "Password manager"
    ]
  },
  {
    "objectID": "pages/support/gestionnaire_mots_de_passe.html#multi-device-synchronization",
    "href": "pages/support/gestionnaire_mots_de_passe.html#multi-device-synchronization",
    "title": "Digital Ecology Scheme (DEN) Resources and Materials",
    "section": "4 - Multi-device synchronization",
    "text": "4 - Multi-device synchronization\nFor this last part, we’re going to configure our database so that it can synchronize across multiple devices. This part is slightly more technical, so it’s important not to get discouraged and, to put it simply, “let go of the railing”. Don’t forget that you can always come back to me if you need help.\nIn this example, we’re going to set up two configurations that will probably meet most of your needs:\n\nsynchronization between a computer and a smartphone,\nsynchronization between two computers.\n\nIn addition, you’ll note that the examples below were created using Windows for the computers and Android for the smartphone. Where this has been identified, alternatives to other OSes will be listed, but there is no guarantee that they have been tested to the full extent of the procedure’s objective.\n\n4.1 - Syncthing software\nThe first step is to install software on our devices that will enable us to perform our synchronizations. To do this, we’ll be using Syncthing which is a free, open-source program that synchronizes files between devices in real time, without the need for a central server. It’s private, secure, encrypted and easy to use.\nImportantly, as mentioned above, this software does not operate via a central server. In practice, this means that during synchronization processes, the devices must be connected at the same time. In the case of two computers, this condition can be complicated to meet, especially if we use one at work and the second at home for telecommuting. That’s why, in our configuration, our phone will be the equivalent of our “central server”, as it will have “more chance” of being connected at the same time as the computers.\nOn the application’s official website, go to the “Downloads” section. You’ll find that there are several distributions available, depending on your operating system.\nAs a guide, the process for installing it on a Linux system can be found at next. Follow the instructions and run the associated command lines. You should have two shortcuts to the application:\n\nStart Syncthing” to launch the software,\nSyncthing Web UI” takes you to the software configuration page (see next section for details).\n\nFinally, unlike Windows, which automatically launches Syncthing when the OS starts up, you’ll need to configure this automatic startup on Linux (for example, by running the “Start Syncthing” shortcut at each boot).\n\n\n4.2 - Installing and configuring the Syncthing client under Windows\nWe’re going to start by installing it on our computer, selecting the “Syncthing Windows Setup” integration which allows you to install it under Windows. To keep things simple, go to this link and download the .exe executable (figure 25, currently the latest version is 1.29.0).\n\n\n\nFigure 25: Latest version of ’Syncthing Windows Setup\n\n\nDuring installation, if possible with administrator rights, follow these steps:\n\nwhen the “Select Destination Location” window appears, change the path to a “simpler” one, such as “C:\\Syncthing” (figure 26),\n\n\n\n\nFigure 26: Installation folder\n\n\n\nin the “Select Additional Tasks” window, check the “Create desktop shortcut for Syncthing configuration page” box in addition to the boxes already checked (figure 27),\n\n\n\n\nFigure 27: Additional tasks window\n\n\n\nclick yes when prompted to create a firewall rule (figure 28),\n\n\n\n\nFigure 28: Firewall rule\n\n\n\nfinally, check the “Open Syncthing configuration page” box (figure 29).\n\n\n\n\nFigure 29 : Last stand\n\n\nOnce installed, you should see the main application window (figure 30). You can access it via your browser at 127.0.0.1:8384 (if you left these settings untouched during installation) or by clicking on the “Syncthing Configuration Page” shortcut added to your desktop. Depending on your browser, it may inform you that the page you wish to display (127.0.0.1) presents a probable security risk. You can override this warning and display the page anyway (via, for example, the “Advanced” button and something like “Continue to page”).\nOn this first page, the application may advise you to add a username and password to access the interface. From a security point of view, this is a plus, but for the sake of accessibility and assuming that your computer is locked with a password and that you don’t leave your workstation unlocked, we won’t be configuring authenticated access.\n\n\n\nFigure 30: Syncthing global page\n\n\nNow that our software is installed, the next step is to add a share folder that will contain our KeePassXC database. To do this, simply click on the “Add a share” button on the home screen. On the configuration screen (figure 31), we’re going to give it a name (it may be different between devices), as well as a share ID (it must be IDENTICAL between devices) and finally the share path, which in this case will be the location of the folder where my database is stored.\n\n\n\nFigure 31: Setting up a share folder\n\n\n\n\n4.3 - Installing and configuring the Syncthing client on Android\nTo configure Syncthing on our Android smartphone, we’re going to retrieve the application directly from the store. Make sure you get the right one, called “Syncthing-Fork”.\nWhen you launch the application, you must authorize it to access several elements of your phone:\n\nstorage access, which will give it access to your database stored locally on your phone (figure 32)\n\n\n\n\nFigure 32: Phone storage access authorization\n\n\n\nregarding battery optimization, it’s advisable to stop battery optimization (figure 33). Here we need our phone not to automatically close the application, for example if it stays open longer than a “basic” application. It should be noted, however, that theoretically, disabling battery optimization could have an impact on battery life. Of course, this will have to be taken into account via other variables, such as the spacing between synchronizations or the type of connection (via mobile data or wifi, for example) but for the moment via my demanding usage example (synchronization still functional via mobile data), the impact in my opinion is not a significance on my battery life.\n\n\n\n\nFigure 33: Battery optimization stop authorization\n\n\n\nfor location permissions, it is also advisable to grant them (figure 34). As indicated in the comments, localization will only be used for wifi network selection, if you wish to perform synchronizations only on predefined wifi networks (your home network, for example).\n\n\n\n\nFigure 34: Location authorization\n\n\n\nfinally, also enable application notifications (figure 35) so that the application can inform you of synchronization details.\n\n\n\n\nFigure 35: Authorization notifications\n\n\nNow that the application is correctly installed, by default the conditions for executing synchronizations are set to take place over wifi only. For my personal use, and in order to test my synchronization processes directly, I chose to activate the synchronization executor from mobile data (via 4G or 5G, for example). You’re free to do that too (bearing in mind the impact it can have on battery life, but above all on your mobile plan), but in any case I leave “Run while roaming” unchecked so that synchronization doesn’t take place when I’m abroad, for example, where mobile data can be deducted significantly from my plan (the software will go into standby mode at that point). To configure this simply go to the “Settings” tab via the 3 bars at the top left, click on “Execution conditions” (figure 36), then check/uncheck the corresponding boxes (figure 37).\n\n\n\n\n\nFigure 36: Synchronization settings\n\n\n\n\n\n\nFigure 37: Mobile data authorization\n\n\n\n\nStill in the application settings, it would be a good idea to check the “Autostart” box (figure 38) in the “Behavior” section, so that it automatically launches the synchronization process as soon as the phone is switched on.\n\n\n\nFigure 38: Automatic application startup\n\n\n\n\n4.4 - Pairing a computer with an Android phone\nL’application correctement installée et configurée sur notre ordinateur et notre téléphone, il nous reste maintenant à appareiller nos appareils. The easiest way to do this is to use QR codes for identification. On your computer where we have configured Syncthing (section 4.2), open the configuration window (default address 127.0.0.1:8384 or “Syncthing Configuration Page” shortcut on the desktop) and click on the “Actions” tab, then on “Show my ID” (figure 39).\n\n\n\nFigure 39: QR code display\n\n\nReturn to the Syncthing application on your phone, click on the “DEVICES” tab and then on the top-right button with a + to add a new device. In the new window, click on the QR code icon in the top right-hand corner, and allow the application to access your camera (figure 40).\n\n\n\nFigure 40: Camera access authorization\n\n\nGive this device a name (here I’ve chosen “pc_cnrs”) and check the “Initiator” box (figure 41).\n\n\n\nFigure 41: Add computer\n\n\nNow you should see an add request on your computer’s configuration page (figure 42).\n\n\n\nFigure 42: Add computer request\n\n\nAccept it by clicking on “Add device”. In the “General” tab, give it a friendly name (for my example, it will be “portable_android_mat”, figure 43).\n\n\n\nFigure 43: Request add telephone\n\n\nThen click on the “Shares” tab and check the box corresponding to the share folder we created earlier (figure 44).\n\n\n\nFigure 44: Phone sharing authorization\n\n\nNow, in the same way as on our computer (section 4.2), we need to create a shared folder containing our database on our phone. To do this, return to our phone, click on the “SHARES” tab, create a new share using the + button in the top right-hand corner, and configure the parameters. Please note :\n\nas before, you can give it any name you like (here it’s “KeePass”),\nlikewise, you must enter the same identifier as your shared folder on your computer. In my case, it’s “keepass”,\nyou need to enter a location on your phone where your database will be stored. To do this, click on the little cogwheel. For my example and personal configuration, I’ll be in the “/storage/emulated/0/DCIM/KeePass” folder (I’ve created the “KeePass” folder with the little + at top right),\nI’ve activated sharing with the “pc_cnrs” device, which is the pc I paired previously.\nthen validate with the icon at top right.\n\nFor your information, you should have a configuration like figure 45.\n\n\n\nFigure 45: Creating phone shares\n\n\nCongratulations, you should be able to pair and synchronize your folders (figure 46 and figure 47).\n\n\n\n\n\nFigure 46: Sychronization on Android\n\n\n\n\n\n\nFigure 47: Computer synchronization\n\n\n\n\n\n\n4.5 - Pairing two computers\nThis section is a bonus, but can be useful, for example, if you want to synchronize your database with two computers, one at your workplace and the second at home, for telecommuting.\nNow that you’ve synchronized a computer and a cell phone, this step is child’s play:\n\ninstall and configure the Synchting client under Windows on your second computer (section 4.2). Don’t forget that the share ID must be identical to the one set on the other computer and by association with your cell phone (in my example it will be “keepass”),\nrepeat the step in section 4.5 with the second computer.\n\nOnce you’ve done this, you should see your second device next to the first (figure 48).\n\n\n\nFigure 48: Second computer synchronization",
    "crumbs": [
      "Useful links",
      "Global support",
      "Password manager"
    ]
  },
  {
    "objectID": "pages/support/gestionnaire_mots_de_passe.html#installing-the-keepass-application-on-android",
    "href": "pages/support/gestionnaire_mots_de_passe.html#installing-the-keepass-application-on-android",
    "title": "Digital Ecology Scheme (DEN) Resources and Materials",
    "section": "5 - Installing the KeePass application on Android",
    "text": "5 - Installing the KeePass application on Android\nFirst of all, congratulations on getting this far, you’ve done the hard part. In this last section, we’re going to install an Android application on our phone that will enable us to access our database on the phone, and also make it easier for us to fill in passwords on websites (same principle as in section 3 for our computer).\nIn concrete terms, you can choose between :\n\nKeePassDX et KeePass2Android for Android,\nStrongbox and KeePassium for iOS.\n\nIn our example, I’m going to use KeePassDX, which I think has the advantage of a slightly nicer interface than KeePass2Android.\n\n5.1 - Installing and configuring the KeePassDX application\nOnce you’ve installed the KeePassDX app on your Android phone, you should get this first window (figure 49).\n\n\n\nFigure 49 : KeePassDX\n\n\nSimply click on the “Open an existing safe” tab and select your database. In my example, you’ll find it in the “/storage/emulated/0/DCIM/KeePass” folder. Enter your password (figure 50) and accept the notification authorization (figure 51 et 52).\n\n\n\n\n\nFigure 50: Unlocking with password\n\n\n\n\n\n\nFigure 51: Authorization notifications\n\n\n\n\n\n\nFigure 52: Authorization notifications bis\n\n\n\n\nCongratulations! You now have access to your database. Just a word of warning: check your phone’s notifications pane to make sure your database is locked if you no longer need it. By default, it should lock automatically on certain actions (such as locking the screen), but this will depend on your settings.\nBefore we head off for a coffee, we’ve just got a few configurations to make to the application to simplify our interactions with it.\nA good idea is to enable biometric unlocking of the application, which will allow us to unlock it using our fingerprint, for example, instead of our password. To do this, go to the application settings (click on the 3 bars at the top left, then on the “Settings” tab), section “Device unlocking” then check the boxes “Biometric unlocking” and “Open in.. automatically”. (figure 53). The next time you connect to the base, you’ll be asked for your password once again, and all you have to do is click on “Link with device unlock” at the bottom (figure 54) to complete the biometric association (figure 55).\n\n\n\n\n\nFigure 53: Biometric unlocking\n\n\n\n\n\n\nFigure 54: First biometric unlocking\n\n\n\n\n\n\nFigure 55: Functional biometric unlocking\n\n\n\n\nThe second interesting option is the activation of the “Magiclavier”, which will greatly assist the automatic filling of website fields from our phone. To activate it, go once again to Settings, but in the “Form filling” tab. In this section, click on “Device keyboard settings”, select the authorization for the “Magic keyboard (KeePassDX)”, go back and in the “Magic keyboard settings” section, check the “Authentication screen” box in the “Keyboard change” sub-section. (figure 56).\n\n\n\nFigure 56: Magiclavier settings\n\n\nNow, when you get to a site where you have to fill in your credentials, you can change your keyboard using the button at the bottom right of your keyboard. (figure 57), this will give you access to a menu connected to your database, making it easier to fill in the required fields. (figure 58).\n\n\n\n\n\nFigure 57: Switching keyboards\n\n\n\n\n\n\nFigure 58 : Magiclavier",
    "crumbs": [
      "Useful links",
      "Global support",
      "Password manager"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Welcome!\nThe aim of this site is to centralize and provide easy access to informatics resources, procedures and other support for the work of personnel and associates of the UMR MARBEC. This structuring is based in particular on the Dispositif d’Ecologie Numérique, or DEN, which is a transversal entity associated with the UMR. Its missions are to set up, coordinate and share technical resources, and to exchange methodologies and new approaches in support of the digital aspects of scientific research.\nThere’s also an Issues section where you can, for example, report a problem in the site’s source code, or suggest an improvement or new content. These “GitHub Issues” are really to be seen as objects closely linked to a “to-do list” items, and are focused on tasks to be accomplished (for example, throught the creation of a branch dedicated to the subject).\nIn addition, you’ll find a discussion forum to exchange ideas on common topics or issues. Discussions are intended for conversations that need to be transparent and accessible, but do not need to be followed up on a project and are not code-related, unlike “GitHub Issues”.\nFor your information, UMR also has a Rocket chat server accessible through the following URL https://tchat.ird.fr/home. It is possible to access the workspace directly from the URL or by installing a heavy client (=software) on your computer and adding the URL in the “add workspace” section.\nFurthermore, to facilitate access and use by many as possible, you’ll find this website and related resources in French (by default) but also in English (use the button on the left of the search bar to switch language).\nFeel free to visit the “I want to contribute!” section if you have resources to contribute to those available, or even more generally if you want to contribute to the provision of common resources.\nIf you have any specific requirements, please contact the DEN representatives at marbec-den-admin@listes.ird.fr.\nFor your information, this site was generated using the Quarto publishing system.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "pages/support/index_support.html",
    "href": "pages/support/index_support.html",
    "title": "Digital Ecology Scheme (DEN) Resources and Materials",
    "section": "",
    "text": "This section is dedicated to procedures and processes for providing global support to UMR. These include support in configuring for several software (such as email clients), as well as more general processes such as setting up a daily backup solution or suggestions for password management.\n\n\n\n Back to top",
    "crumbs": [
      "Useful links",
      "Global support"
    ]
  },
  {
    "objectID": "pages/packages_logiciels/index_packages_logiciels.html",
    "href": "pages/packages_logiciels/index_packages_logiciels.html",
    "title": "Digital Ecology Scheme (DEN) Resources and Materials",
    "section": "",
    "text": "In this section you’ll find all the packages, software and general informatics resources resources developed or used by UMR MARBEC and associated partners.\n\n\n\n Back to top",
    "crumbs": [
      "Useful links",
      "Packages and software"
    ]
  },
  {
    "objectID": "pages/packages_logiciels/osmose.html",
    "href": "pages/packages_logiciels/osmose.html",
    "title": "Osmose model",
    "section": "",
    "text": "OSMOSE is a multispecies and Individual-based model (IBM) which focuses on fish species. This model assumes opportunistic predation based on spatial co-occurrence and size adequacy between a predator and its prey (size-based opportunistic predation). It represents fish individuals grouped into schools, which are characterized by their size, weight, age, taxonomy and geographical location (2D model), and which undergo major processes of fish life cycle (growth, explicit predation, natural and starvation mortalities, reproduction and migration) and fishing exploitation.\nThe model needs basic biological parameters that are often available for a wide range of species, and which can be found in FishBase for instance, and fish spatial distribution data. This package provides tools to build a model and run simulations using the OSMOSE model.\nThe model is available on GitHub.",
    "crumbs": [
      "Useful links",
      "Packages and software",
      "Osmose model"
    ]
  },
  {
    "objectID": "pages/packages_logiciels/osmose.html#object-oriented-simulator-of-marine-ecosystems",
    "href": "pages/packages_logiciels/osmose.html#object-oriented-simulator-of-marine-ecosystems",
    "title": "Osmose model",
    "section": "",
    "text": "OSMOSE is a multispecies and Individual-based model (IBM) which focuses on fish species. This model assumes opportunistic predation based on spatial co-occurrence and size adequacy between a predator and its prey (size-based opportunistic predation). It represents fish individuals grouped into schools, which are characterized by their size, weight, age, taxonomy and geographical location (2D model), and which undergo major processes of fish life cycle (growth, explicit predation, natural and starvation mortalities, reproduction and migration) and fishing exploitation.\nThe model needs basic biological parameters that are often available for a wide range of species, and which can be found in FishBase for instance, and fish spatial distribution data. This package provides tools to build a model and run simulations using the OSMOSE model.\nThe model is available on GitHub.",
    "crumbs": [
      "Useful links",
      "Packages and software",
      "Osmose model"
    ]
  },
  {
    "objectID": "pages/liens.html",
    "href": "pages/liens.html",
    "title": "Digital Ecology Scheme (DEN) Resources and Materials",
    "section": "",
    "text": "UMR MARBEC\nGitHub repository issues\nGitHub repository discussion forum\nUMR rocket chat server URL\nDEN admin ressource contact\n\n\n\n\n Back to top"
  },
  {
    "objectID": "pages/calendrier/calendrier.html",
    "href": "pages/calendrier/calendrier.html",
    "title": "Schedule",
    "section": "",
    "text": "Schedule\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Useful links",
      "Schedule"
    ]
  },
  {
    "objectID": "pages/git/index_git.html",
    "href": "pages/git/index_git.html",
    "title": "Digital Ecology Scheme (DEN) Resources and Materials",
    "section": "",
    "text": "In this section you will find all the resources related to the use of version control systems. Several forges are used at UMR:\n\nGitHub MARBEC\nGitLab IRD\nGitLab Ifremer\n\n\n\n\n Back to top",
    "crumbs": [
      "Useful links",
      "Version Control System"
    ]
  },
  {
    "objectID": "pages/serveurs/marbec_data/manage_files.html",
    "href": "pages/serveurs/marbec_data/manage_files.html",
    "title": "Managing files from/to marbec-data",
    "section": "",
    "text": "Image credits: Declan Sun at Unplash",
    "crumbs": [
      "Useful links",
      "Servers",
      "Marbec DATA",
      "Manage Files"
    ]
  },
  {
    "objectID": "pages/serveurs/marbec_data/manage_files.html#create-a-shared-work-folder",
    "href": "pages/serveurs/marbec_data/manage_files.html#create-a-shared-work-folder",
    "title": "Managing files from/to marbec-data",
    "section": "Create a shared work folder",
    "text": "Create a shared work folder\n[Content in preparation]",
    "crumbs": [
      "Useful links",
      "Servers",
      "Marbec DATA",
      "Manage Files"
    ]
  },
  {
    "objectID": "pages/serveurs/marbec_data/manage_files.html#linking-a-working-folder-to-marbec-gpu.",
    "href": "pages/serveurs/marbec_data/manage_files.html#linking-a-working-folder-to-marbec-gpu.",
    "title": "Managing files from/to marbec-data",
    "section": "Linking a working folder to marbec-gpu.",
    "text": "Linking a working folder to marbec-gpu.\n[Content in preparation]",
    "crumbs": [
      "Useful links",
      "Servers",
      "Marbec DATA",
      "Manage Files"
    ]
  },
  {
    "objectID": "pages/serveurs/marbec_data/manage_files.html#managing-files-between-marbec-data-and-our-pc-filezilla",
    "href": "pages/serveurs/marbec_data/manage_files.html#managing-files-between-marbec-data-and-our-pc-filezilla",
    "title": "Managing files from/to marbec-data",
    "section": "Managing files between marbec-data and our PC (FileZilla)",
    "text": "Managing files between marbec-data and our PC (FileZilla)\n\nInstalling FileZilla and connecting to marbec-data.\nThe easiest way to move (copy, cut and paste) files from our PC to one of our shared work folders or to our marbec-gpu user folder is through the (free) FileZilla software. To download the installer, just go to its official website https://filezilla-project.org/ and select the Download FileZilla Client button.\n\nThen, by default we will be offered to download the version corresponding to the operating system (OS) where we are running our browser, but we can always choose the most appropriate version in the section More download options.\n\n\n\n\n\n\n\nOperating systems and CPU architectures\n\n\n\nIn recent years, processors with ARM architecture have been incorporated into the PC market. The most recent and famous example is Apple’s Mx series (e.g. M1); however, in recent months laptops with ARM processors (from the Snapdragon brand, for instance) have also appeared. Software compiled for an ARM architecture will not work on an x86 architecture (which is the architecture manufactured by brands such as Intel or AMD) and vice versa, so it will always be important to know not only which OS our PC is running (Windows, MacOS or Linux), but also the architecture of our processor.\n\n\nOnce the file has been downloaded, it will be enough to run it leaving most of the options by default (except those that offer us to install some additional program that we do not need, e.g. Chrome). After that, we will be able to run the program and we will obtain an environment that will look like this:\n\nThe next thing we will do is to establish a connection to marbec-data. To do this, at the top, we will fill in the following fields:\n\nServer: marbec-data.ird.fr\nUser: youruser\nPassword: yourpassword\nPort: 22\n\nIf all goes well, a message indicating that the connection has been successful will be displayed in the panel immediately below. In addition, the next two lower panels to the right will show those folders already linked and available in our marbec-data account.\n\n\n\n\n\n\n\nNote\n\n\n\nIt is not necessary to log in every time we log back into FileZilla. We could save our login and skip the above steps by clicking the small arrow to the right of Quick Login and selecting our saved login. Of course, allowing our login credentials to be saved should ONLY occur on our personal PC.\n\n\n\nAnd that is all! In the left panels, we will be able to navigate in the directories of our PC, while in the right panels we will be able to do it in the marbec-gpu and marbec-data ones.\n\n\nCopying files and folders\nIt will be as simple as dragging the element between the left and right panels. The process will start and the bottom pane (the last one) will show the queued, completed and failed transfers.\n\nAlso, if at any time FileZilla detects that there are repeated items, it will show a small window with multiple options available (overwrite and skip, verify differences in sizes or names, apply the selected option to future cases in the transfer queue, etc.).",
    "crumbs": [
      "Useful links",
      "Servers",
      "Marbec DATA",
      "Manage Files"
    ]
  },
  {
    "objectID": "pages/serveurs/marbec_data/manage_files.html#manage-files-within-marbec-data.",
    "href": "pages/serveurs/marbec_data/manage_files.html#manage-files-within-marbec-data.",
    "title": "Managing files from/to marbec-data",
    "section": "Manage files within marbec-data.",
    "text": "Manage files within marbec-data.\nWhile the marbec-data web environment explorer offers the options to copy, paste, delete, etc., it is not an efficient method when our files are medium or large (&gt;10 MB). Here is how to perform these operations from Terminal.\n\nCopy-paste\nFor this, the simplest way is through the cp command and making use of the navigation commands cited in this post (e.g. .. to indicate a previous folder). The basic syntax is the following: cp path/origin /path/destination, but there are different possible cases:\n\nCopy a file into the same folder, but with a different name (create duplicate): cp file1.csv file1-dup.csv.\nCopy a file to another folder: cp path/file1.csv path/destination.\nCopy more than one file to another folder: cp path/file1.csv path/file2.csv folder/destination\nCopy a folder to another folder: cp path/folder1 path/folder2 --recursive or cp path/folder1 path/folder2 -r.\n\n\n\n\n\n\n\nNote\n\n\n\nBy default, cp will overwrite any file with the same name. To avoid this, it is possible to add the -n option as follows: cp path/from/file1.csv path/destination -n.\n\n\n\n\nCut-paste (and also rename)\nIt will be very similar to the above, but through the mv command:\n\nRename a file (within the same folder): mv file1.csv file2.csv\nMove a file to another folder: mv path/file1.csv path/to/destination\nMove one file to another folder: mv path/file1.csv path/file2.csv path/destination\nMove one folder to another folder: mv path/old/folder path/new/folder\n\n\n\nDelete\nFor this, we will use the rm command as follows:\n\nDelete a file: rm path/to/file.csv\nDelete a folder (and all its contents): rm path/to/folder -r\n\n\n\n\n\n\n\nNo turning back\n\n\n\nWhile inside Terminal it is always possible to cancel a command using the shortcut Ctrl+C (or Cmd+C on MacOS), once the rm command completes its work, there is no way to revert the deletion or recover it from a recycle garbage can, so be very careful when using it.",
    "crumbs": [
      "Useful links",
      "Servers",
      "Marbec DATA",
      "Manage Files"
    ]
  },
  {
    "objectID": "pages/serveurs/marbec_gpu/basic_example.html",
    "href": "pages/serveurs/marbec_gpu/basic_example.html",
    "title": "Running a simple example on Marbec-GPU",
    "section": "",
    "text": "Throughout this tutorial, you will find everything you need to run a Python or R script (displaying “Hello World &lt;3”), along with additional tips and resources that will be helpful for other MARBEC-GPU tasks.\nThere are two main ways to run a program on Marbec-GPU. The first is to use a task submission script, the second is to use the session interactively. In this example, we will use the first method which is by far the most suitable and easily adaptable for more complex programs.\n\n\n\n\n\n\nNote\n\n\n\nWhen you want to run a more complex program, make sure your project works locally (on your personal computer). This means setting up your environment correctly and debugging your script locally. Once everything works successfully on your PC (even using only 1% of the dataset if you encounter computational constraints), you can then deploy your project on MARBEC-GPU.\n\n\nStart by creating a working directory in which the different files will be created. In bash command it would be :\ncd ~  # go to the home directory\nmkdir my_project  # create a folder for the project\nOtherwise you can use the Jupyter interface to create a working directory with the icon framed in red below:\n\n\n\nCreate a working directory\n\n\nThen move to this directory ( cd my_project/ ).",
    "crumbs": [
      "Useful links",
      "Servers",
      "Marbec GPU",
      "SLURM Submission : Basic R/Python example"
    ]
  },
  {
    "objectID": "pages/serveurs/marbec_gpu/basic_example.html#prepare-the-python-or-r-script",
    "href": "pages/serveurs/marbec_gpu/basic_example.html#prepare-the-python-or-r-script",
    "title": "Running a simple example on Marbec-GPU",
    "section": "1. Prepare the Python or R Script",
    "text": "1. Prepare the Python or R Script\nCreate a simple Python or R script that displays “Hello World &lt;3”. Here is an example of a script:\nprint(\"Hello World &lt;3\")\nSave this script in a file named main.py or main.R (depending on the desired language) in the working directory you created earlier.",
    "crumbs": [
      "Useful links",
      "Servers",
      "Marbec GPU",
      "SLURM Submission : Basic R/Python example"
    ]
  },
  {
    "objectID": "pages/serveurs/marbec_gpu/basic_example.html#prepare-a-bash-script-with-slurm-arguments",
    "href": "pages/serveurs/marbec_gpu/basic_example.html#prepare-a-bash-script-with-slurm-arguments",
    "title": "Running a simple example on Marbec-GPU",
    "section": "2. Prepare a Bash Script with SLURM Arguments",
    "text": "2. Prepare a Bash Script with SLURM Arguments\nIn order to run the script correctly, you will need to create a bash script launch.sh taking care to mention:\n\nthe SLURM arguments, specifying which resources to allocate, the job name, the output file, etc.\nPython/R script execution.\n\nHere is a minimal example of a bash script:\n#!/bin/bash\n\n#SBATCH --job-name=my_job         # Job name\n#SBATCH --output=job_%j.out`      # Standard output and error log\n#SBATCH --gres=gpu:1             # Number of GPUs (Remove the line if no GPU is required.)\n#SBATCH --mem=4G                  # Memory allocation (4 GB)\n#SBATCH -c 1                      # Number of CPU cores\n\n# execute python file\npython main.py                    \n\n# execute R file\nRscript main.R\nIt is possible to specify other SLURM arguments. For more information on SLURM arguments, you can consult the official SLURM documentation here.",
    "crumbs": [
      "Useful links",
      "Servers",
      "Marbec GPU",
      "SLURM Submission : Basic R/Python example"
    ]
  },
  {
    "objectID": "pages/serveurs/marbec_gpu/basic_example.html#execute-the-bash-script",
    "href": "pages/serveurs/marbec_gpu/basic_example.html#execute-the-bash-script",
    "title": "Running a simple example on Marbec-GPU",
    "section": "3. Execute the Bash Script",
    "text": "3. Execute the Bash Script\nAfter the previous 2 steps, the working directory should contain the following files: launch.sh and main.py/main.R:  The final step is to submit your launch.sh script created in the previous section. To do this, you need to use the sbatch command (see documentation).\nIn the terminal, run the following command:\n\nsbatch launch.sh\n\nIf the SLURM parameters (#SBATCH arg) are correctly filled in, you should see a job submission confirmation message: Submitted batch job 1234567. Otherwise, an error message will appear instead. Upon successful submission, SLURM checks the requested resources and places the job in the queue (state PENDING) until the resources become available. Once the resources are available, the job runs (state RUNNING). An output file is then created in the current directory with the name specified in the bash script (#SBATCH --output=job_%j.out). A second file containing error messages can appear if specified (#SBATCH --error=job_%j.err).\nYou can track the progress of your job with the squeue -u $USER or squeue -j 1234567 command (with 1234567 being your job number). You can also list all jobs currently running or in the queue with squeue -O NAME,UserName,TimeUsed,tres-per-node,state,JOBID. The STATE column in particular indicates the job’s status (PENDING RUNNING). For more details on the squeue command, you can consult the documentation.\nTo cancel a job (running or still in the queue), use the scancel 1234567 command (with 1234567 being your job number).\nThe output.log file containing the outputs of your python script is created in the current directory. You can view it with the cat output.log command or simply by double-clicking on it. It should look like this:\nHello World &lt;3",
    "crumbs": [
      "Useful links",
      "Servers",
      "Marbec GPU",
      "SLURM Submission : Basic R/Python example"
    ]
  },
  {
    "objectID": "pages/serveurs/marbec_gpu/marbec-data_folders.html",
    "href": "pages/serveurs/marbec_gpu/marbec-data_folders.html",
    "title": "🚀 Accessing MARBEC-DATA Folders in JupyterHub",
    "section": "",
    "text": "The MARBEC-DATA server is dedicated to data storage and is connected to the MARBEC-GPU server, which is used for computations.\nThe folders stored in MARBEC-DATA are always accessible via the command line.\n🔍 You can view the available folders by typing:\nls /marbec-data/\nor navigate directly to a specific folder:\ncd /marbec-data/&lt;your_folder&gt;\nHowever, these folders do not appear directly in the file tree.\nTo view them in the Jupyter file explorer, you need to create a symbolic link.\n\n\n\nOpen a terminal in JupyterHub.\nNavigate to the location where you want the folder to appear:\n\ncd /home/your_username/\n\nCreate the symbolic link with the name of your folder in MARBEC-DATA:\n\nln -s /marbec-data/&lt;your_folder&gt;\n\nRefresh the Jupyter interface to see the folder appear.",
    "crumbs": [
      "Useful links",
      "Servers",
      "Marbec GPU",
      "Accessing MARBEC-DATA Folders"
    ]
  },
  {
    "objectID": "pages/serveurs/marbec_gpu/marbec-data_folders.html#creating-a-symbolic-link",
    "href": "pages/serveurs/marbec_gpu/marbec-data_folders.html#creating-a-symbolic-link",
    "title": "🚀 Accessing MARBEC-DATA Folders in JupyterHub",
    "section": "",
    "text": "Open a terminal in JupyterHub.\nNavigate to the location where you want the folder to appear:\n\ncd /home/your_username/\n\nCreate the symbolic link with the name of your folder in MARBEC-DATA:\n\nln -s /marbec-data/&lt;your_folder&gt;\n\nRefresh the Jupyter interface to see the folder appear.",
    "crumbs": [
      "Useful links",
      "Servers",
      "Marbec GPU",
      "Accessing MARBEC-DATA Folders"
    ]
  },
  {
    "objectID": "pages/serveurs/marbec_gpu/basic_command.html",
    "href": "pages/serveurs/marbec_gpu/basic_command.html",
    "title": "Main commands in marbec-gpu Terminal",
    "section": "",
    "text": "Main commands in marbec-gpu Terminal\nThe first thing to keep in mind is that marbec-gpu has Ubuntu installed, so the commands listed below will be the same as the ones used in that OS. This article will show a description of the main usage modes for each command, but if you have any additional requirements, you can always search in forums like Stackoverflow or check the help for each command, which consists of placing the command name followed by --help. For example, if I want to know the help for the ls command, just run ls --help in the Terminal.\n\n\n\n\n\n\nUpper and lower case\n\n\n\nAs in R or Python, the use of upper or lower case when indicating an option does matter. For example, ls -D is not equivalent to ls -d, so be carefull.\n\n\n\nBrowsing within folders\n\nCommand: cd\nUsage: cd path/folder\n\nTo indicate a previous position (folder), you will use the statement .. as follows: ../path/folder1 (this indicates that there is a folder called path from the folder where you are, and that that has a folder called folder1 as well).\n\n\nCreate a folder\n\nCommand: mkdir\nUsage: mkdir path/folder\n\n\n\nGet the content of a folder as a list\n\nCommand: ls\nUsage: ls path/folder/\n\nMain options:\n\n--all (o -a): Displays all files and subfolders, including those protected (hidden) by the system.\n\n\n\nGenerate a list of files/folders and display the size of each item\n\nCommand: du\nUsage: du path/to/file.csv o du path/to/folder\n\nMain options:\n\n--human-readable (o -h): changes the units dynamically to avoid displaying all Kb. This is especially useful when you have large objects (subfolders or files).\n--summary (o -s): displays a summary table, i.e. it only includes the subfolders and files present at the first search level. This is useful when we just want to take a quick look and avoid displaying a complete listing of ALL internal subfolders.\n\nIf I want to get a list of all the files and folders inside a folder with their respective sizes (the three options are equivalent):\ndu ruta/de/folder/* --human-readable --summarize\ndu ruta/de/folder/* -h -s\ndu ruta/de/folder/* -hs\n\n\nCopy-paste\nFor this, the simplest way is through the cp command and making use of the navigation commands cited in this post (e.g. .. to indicate a previous folder). The basic syntax is the following: cp path/origin /path/destination, but there are different possible cases:\n\nCopy a file into the same folder, but with a different name (create duplicate): cp file1.csv file1-dup.csv.\nCopy a file to another folder: cp path/file1.csv path/destination.\nCopy more than one file to another folder: cp path/file1.csv path/file2.csv folder/destination\nCopy a folder to another folder: cp path/folder1 path/folder2 --recursive or cp path/folder1 path/folder2 -r.\n\n\n\n\n\n\n\nNote\n\n\n\nBy default, cp will overwrite any file with the same name. To avoid this, it is possible to add the -n option as follows: cp path/from/file1.csv path/destination -n.\n\n\n\n\nCut-paste (and also rename)\nIt will be very similar to the above, but through the mv command:\n\nRename a file (within the same folder): mv file1.csv file2.csv\nMove a file to another folder: mv path/file1.csv path/to/destination\nMove one file to another folder: mv path/file1.csv path/file2.csv path/destination\nMove one folder to another folder: mv path/old/folder path/new/folder\n\n\n\nDelete\nFor this, we will use the rm command as follows:\n\nDelete a file: rm path/to/file.csv\nDelete a folder (and all its contents): rm path/to/folder -r\n\n\n\n\n\n\n\nNo turning back\n\n\n\nWhile inside Terminal it is always possible to cancel a command using the shortcut Ctrl+C (or Cmd+C on MacOS), once the rm command completes its work, there is no way to revert the deletion or recover it from a recycle garbage can, so be very careful when using it.\n\n\n\n\nDisplay current processes\n\nCommand: top\n\nWhen you run it, it will show in interactive mode in Terminal the processes that are running, as well as the resources used by each of them (basically like a Task Manager). To exit this interactive mode, just press the q key.\n\n\nStop a process\nIf we want to force the closing or the cancellation of a process already started, we can use the shortcut Ctrl+C (or Cmd+C in MacOS). It is important to keep in mind that forcing the closing of a process that had in progress the handling of files or folders (creation, copy, etc.) can leave the generated files unusable.\n\n\nViewing a plain text file\nBy default, there are two tools available from Terminal: vi and nano. The syntax for their execution is as simple as vi path/file1.txt or nano path/file1.txt, where file1.txt can be any plain text file (e.g. an R or Python script). The navigation shortcuts within each of these environments are different, but documentation is abundant on the Internet. Choose the one you like best.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Useful links",
      "Servers",
      "Marbec GPU",
      "Useful Commands Guide"
    ]
  },
  {
    "objectID": "pages/formations/index_formations.html",
    "href": "pages/formations/index_formations.html",
    "title": "Digital Ecology Scheme (DEN) Resources and Materials",
    "section": "",
    "text": "This section is dedicated to referencing the training courses and associated materials made available to UMR and its partners. Due to the specific nature of the training programs, the associated materials may not be available in several languages. In addition, by following this link to be defined you will find a list of the training courses organized by the UMR and, above all, the dates on which they will take place.\nFor your information, most of the following subsections link to Git repositories where you can find training resources. If you have any questions relating to these, it’s best to use the services provided by the repository (the Issues section, for example) or, alternatively, to contact the associated contact person(s) directly.\n\n\n\n Back to top",
    "crumbs": [
      "Useful links",
      "Training"
    ]
  },
  {
    "objectID": "pages/formations/r_package_developpement.html",
    "href": "pages/formations/r_package_developpement.html",
    "title": "R package development training",
    "section": "",
    "text": "R package development training\n\n\nThe training source code is available on the GitHub repository. The associated book presentation is available at the following address.\nThe training program is currently being set up and should be finalized during 2025.\nThe first training session is scheduled for late 2025 at the latest.\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Useful links",
      "Training",
      "R package development training"
    ]
  }
]