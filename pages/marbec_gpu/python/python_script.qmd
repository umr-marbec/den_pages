---
format:
  html:
    css: ../../../styles.css
author: "[Auguste Verdier](mailto:auguste.verdier@umontpellier.fr)"
date: "2025-01-13"
---

::: {.content-visible when-profile="french"}
# Script Python 

Pour ce tutoriel, nous allons simplement vérifier si Pytorch accède au GPU. Pour cela il suffit de créer dans le dossier courant un fichier python `main.py`.
Voici le contenu du fichier `main.py` :


```{.python}
import torch
import torchvision.models as models
import os

def main():
    # Vérifier si un GPU est disponible
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Device utilisé : {device}")

    # Charger un modèle CNN 
    model = models.resnet152()
    
    # Envoyer le modèle sur le GPU (si disponible)
    model = model.to(device)

    # Afficher le pourcentage de mémoire GPU occupé si un GPU est utilisé
    if torch.cuda.is_available():
        total_memory = torch.cuda.get_device_properties(0).total_memory
        reserved_memory = torch.cuda.memory_reserved(0)
        allocated_memory = torch.cuda.memory_allocated(0)
        free_memory = reserved_memory - allocated_memory

        print(f"Mémoire GPU totale : {total_memory / 1e9:.2f} GB")
        print(f"Mémoire GPU allouée : {allocated_memory / 1e9:.2f} GB")
    

    os.system("nvidia-smi")

if __name__ == "__main__":
    main()
```

:::

::: {.content-visible when-profile="english"}
# Python Script

For this tutorial, we will simply check if Pytoch can acces to the GPU. To do this, simply create a python file `main.py` in the current directory.
Here is the content of the file `main.py` :

```python
import torch
import torchvision.models as models
import os

def main():
    # Check if a GPU is available
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Device used : {device}")

    # Load a CNN model
    model = models.resnet152()
    
    # Send the model to the GPU (if available)
    model = model.to(device)

    # Display the percentage of GPU memory used if a GPU is used
    if torch.cuda.is_available():
        total_memory = torch.cuda.get_device_properties(0).total_memory
        allocated_memory = torch.cuda.memory_allocated(0)

        print(f"Total GPU memory : {total_memory / 1e9:.2f} GB")
        print(f"Allocated GPU memory : {allocated_memory / 1e9:.2f} GB")
    
    os.system("nvidia-smi")
```
:::